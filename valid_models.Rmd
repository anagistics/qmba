# Valide Modelle

*Deutsch, damit es als Arbeitsblatt geeignet ist*

In diesem Kapitel geht es um Techniken, um *valide* Modelle zu erstellen. Valide Modelle erfüllen die Voraussetzungen der Methoden, die für ihre Analyse verwendet werden. Wir werden hier zunächst mit Lineare Regression arbeiten, weil diese Methode gut verständlich und theoretisch fundiert ist und ihre Ergebnisse gut *erklärbar* sind. Das heißt nicht, dass diese Method auch immer *geeignet* ist, aber in vielen praktischen Fällen kann sie ein guter erster Ansatz sein.

Wir werden einen im Internet und für R verfügbaren Datensatz benutzen, den *Ames Iowa Housing" Datensatz (s. @R-AmesHousing). Dabei handelt es sich um Verkaufsdaten zu Wohnungen und Gebäuden sowie um beschreibende Daten der Immobilien. Es handelt sich damit um ein durchaus anwendungsnahes Szenario.

## Das Szenatio

Die *HaiBank AG* möchte im Niedrigzinsumfeld und angesichts steigender Immobilienpreise ihr eigenes Angebot als Immobilienmakler ausbauen. Unter der Marke *ImmoHai* sollen Maklerdienste für Immobilien angeboten werden und eine wichtige Komponente dabei ist die Abschätzung der erwarteten Profitabilität der Vermittlung einer Immobilie. Neben dem eigenen Aufwand auf der Kostenseite ist dafür der erzielbare Verkaufspreis maßgeblich. Traditionell wurde dieser Verkaufspreis von Immobilienexperten nach einer Begehung des Objekts und anhand der Erfahrung eingeschätzt. Weil die Unternehmensstrategie einen deutlichen Ausbau des Maklergeschäfts anstrebt, ist dieser Ansatz aufgrund des hohen Aufwands nicht mehr zu leisten. Stattdessen soll es, ähnlich wie bei der Bonitätsprüfung für Hauskredite, eine Software geben, die anhand von Kennzahlen der Immobilie einen realisierbaren Verkaufspreis prognostiziert. Diese Software soll von Sachbearbeitern im Kundengespräch verwendet werden und dabei helfen zu entscheiden, ob die *HaiBank* als Makler für die betreffende Immobilie auftreten soll.

### Aufgabenstellung

Ihre Aufgabe besteht darin, ein **Prognosemodell für Immobilienpreise** und einen Prototyp für die zugehörige **Anwendung** zu entwickeln, die dieses Prognosemodell verwendet.

Weil Ihre hauseigene IT auf absehbare Zeit noch damit beschäftigt ist, die bislang getrennt in jeder Filiale in Excel-Dateien festgehaltenen Immobiliengeschäfte in eine zentrale Datenbank zu überführen, haben Sie sich aus dem Internet eine frei verfügbare Datensammluung von Immobilienpreisen besorgt. Mit Hilfe dieser Daten können Sie schon einmal den ganzen Prozess der Modellentwicklung durchlaufen und einen Prototypen erstellen, damit Ihr Management auch nachvollziegen kann, wie die fertige Lösung aussehen wird.

Als technische Plattform stehen Ihnen 

* die Programmiersprache *R*, 
* die Entwicklungsumgebung *RStudio* und 
* das auf *R* basierende Framework *flexdashboard* 

für die Entwicklung von Modell und Anwendung zur Verfügung.

### Setup

Um mit *R* zu starten bietet es sich an, ein Konto bei *RStudio Cloud* anzulegen. Damit haben Sie eine *RStudio* Entwicklungsumgebung, die Sie über Ihren Browser nutzen können und eine Basisinstallation von *R* als Ausgangsbasis. Im Rahmen dieses Kurses können Sie einem bereits bestehenden *Workspace* beitreten (Anleitung s. unten). 

Alternativ können Sie *R* und *RStudio* auch lokal auf Ihrem Rechner installieren. Wie dies gemacht wird, erfahren Sie unter [R und RStudio installieren](https://rstudio.com/products/rstudio/download/).

#### RStudio Cloud Workspace
Hier die Erklärung, wie das funktioniert

### Grundsätzliches zur Arbeit mit *R*

Neben Python ist *R* eine der derzeit populärsten Sprachen für Datenanalye, Statistik und maschinellem Lernen. Während Python als vielseitige Programmiersprache gerne von einer Community eingesetzt wird, die aus der Softwareentwicklung kommt, hat *R* seinen Ursprung in der statistischen und wissenschaftlichen Community und bietet sehr vielfältige Möglichkeiten für (explorative) Datenanalyse und statistische Methoden. Zu den letzteren kann man im weiteren Sinne auch Methoden des maschinellen Lernens zählen.



## Explorative Datenanalyse

Wenn man eine neue Datenquelle erschließen möchte, bestehen die ersten Schritte aus der *explorativen Datenanalye*. Hierbei geht es darum, ein Verständnis für die Daten zu bekommen und insbesondere Fehler und Auffälligkeiten zu entdecken.

Wir laden zunächst die Immobiliendaten über ein *R*-Paket:
```{r}
library(AmesHousing)
```

Die Daten werden wir unter der Bezeichnung `raw` weiter untersuchen:
```{r}
raw <- ames_raw
```


Um uns einen ersten Eindruck von den Daten zu verschaffen, benutzen wir die Funktion `glimpse` aus dem `dplyr`-Paket. Dieses Paket ist grundsätzlich eine gute Wahl für die Datenanalyse, weil es zahlreiche recht intuitiv anwendbare Funktionen für Datentransformationen, Datenfilterung und -aggregation bietet.

```{r}
library(dplyr)

glimpse(raw)
```

Aus dieser Übersicht erfahren wir schon eine ganze Menge:

* die Zahl der Datensätze
* die Zahl der Merkmale
* die Namen der Merkmale 
* die Datentypen der Merkmale
* eine Vorschau auf die Merksmalsausprägungen.

    **Aufgabe**
    Was bedeuten die Ausprägungen *NA* für die Variable *Alley*. 


    **Aufgabe**
    Was bedeutet die Typkennzeichnung `fct` in der Ausgabe von `glimpse`? Was unterscheidet eine *kategorielle* Variable von numerischen? Was wäre der Unterschied, wenn die Variable *Heating* als Zeichenkette (`char` bzw. `character`) abgebildet würde?
    
## Erstes Verständnis der Daten

Mit `glimpse` konnten wir einen ersten Eindruck der Daten gewinnen. Die zahlreichen Einzelwerte bieten aber noch keinen guten Überblick. Dafür ist eine stärker aggregierte Darstellung besser geeignet. Das Paket `dlookr` hilft dabei:
```{r}
library(dlookr)
```

Angewandt auf unsere Immobiliendaten erhalten wir:
```{r}
diagnose(raw)
```

        **Aufgabe**
        * Was können Sie aus der *unique_rate* von 1 für *PID* ableiten? 
        * Was sagt Ihnen die *unique_rate* von 16 für die Variable *MS SubClass*?
        
### Zusammenfassung numerischer Daten

Wir können zu numerischen Werten mittels der Funktion `diagnose_numeric` mehr Informationen zu den Variablen erhalten, denen ein numerischer Datentyp zu Grunde liegt:

```{r}
diagnose_numeric(raw)
```

### Zusammenfassung kategorieller Daten

Für kategorielle Variablen lassen sich keine sinnvollen Quantile und auch kein Mittelwert angeben. Hier sieht die Zusammenfassung für die Variable *MS Zoning* folgendermaßen aus:
```{r}
diagnose_category(raw, `MS Zoning`)
```

*R* bietet übrigens standardmäßig, d.h. ohne weitere Pakete zu laden, die Funktion `summary`, die eine kompakte Übersicht über einen Datensatz bietet:
```{r}
summary(raw)
```

Aber man sieht schon deutlich den Unterschied: Die Ausgabe über die Funktionen des Pakets `dlookr` ist übersichtlicher und auch leichter weiterzuverarbeiten.

## Fehlende Werte

In vielen Datensätzen, die aus unternehmensinternen IT-Systemen bezogen werden, finden sich Datensätze mit Merkmalen ohne Ausprägung. Es gibt keine pauschale Vorgehensweise, die bzgl. der Behandlung dieser fehlenden Werte immer richtig ist. Zunächst kann es sich lohnen die Spalten (Variablen) in den Datensätzen zu identifizieren, die viele fehlende Werte aufweisen.

Gehen wir nochmal zurück zur obigen Diagnose:
```{r}
diagnose(raw)
```

Die Spalte *missing_percent* gibt den Anteil fehlender Werte an und wir können die Spalten mit häufig fehlenden Werten über eine Sortierung erkennen:
```{r}
diagnose(raw) %>% arrange(desc(missing_percent))
```

Deutlich wird, dass die Variable *Pool QC* wohl häufig keine Werte enthält. Eine Vermutung wäre, dass diese Variable etwas mit dem Vorhandensein eines Pools zu tun hat und dass fehlende Werte schlichtweg bedeuten, dass die Immobilie keinen Pool hat. Etwas anders sieht es bei der Variable *Bsmt Cond* aus. Diese fehlt in knapp 3% aller Fälle -- ebenso wie andere Variablen mit ähnlichem Namen. Vielleicht ist es hier ähnlich wie bei der Variablen *Pool QC* und nicht vorhandene Werte bedeuten einen fehlenden Keller. Aber kann es sein, dass bei ca. 3% aller Immobilien kein Keller vorhanden ist? Um dies besser zu verstehen, müsste man mehr über die Bauweise in der Region in Erfahrung bringen: Vielleicht wurden früher bei der Erfassung der Immobiliendaten keine Angaben zum Keller aufgenommen und die Daten fehlen schlichtweg für ältere Immobilien?

Unabhängig davon, was hier im konkreten Fall Ursache für die fehlenden Werte ist: Bei der explorativen Datenanalyse müssen wir stets berücksichtigen, dass das Fehlen selbst auch eine fachliche Information (hier: *kein Pool vorhanden*) bedeuten kann und nicht immer auf Fehler bei der Datenaufnahme oder -übertragung zurückzuführen ist bzw. mit Änderungen in der Erfassungssystematik zu tun haben muss.

Wir haben durch die obige Analyse schon den Verdacht, dass fehlende Werte für mehrere Variablen in typischen Kombinationen vorkommen: Wenn es keinen Keller gibt oder keine Werte erfasst wurden, dann fehlen wahrscheinlich Werte für alle diesbezüglichen Variablen. Um das zu untersuchen, können wir das Paket `VIM` verwenden:

```{r}
library(VIM)
```

Wenden wir es auf unsere Daten an, wobei wir uns auf die Variablen beschränken, die laut vorangehender Analyse fehlende Werte aufweisen:
```{r}
missing_cols <- diagnose(raw) %>% filter(missing_percent > 0.0) %>% pull(variables)

raw %>% select_at(vars(all_of(missing_cols))) %>% aggr(combined = TRUE, sortVars = TRUE, cex.axis = 0.5)
```

Die für diese Untersuchung gewählte Beschränkung auf Variablen, die fehlende Werte aufweisen, birgt allerdings ein Problem: Auf diese Weise entgehen uns Zusammenhänge zwischen fehlenden Werten und anderen Variablen, die möglicherweise auf einen systematischen Zusammenhang schließen lassen. Vielleicht fehlen ja die Angaben zu Kellern für Gebäude bestimmter Jahrgänge oder aus bestimmten Gebieten, deren geologischen Eigenschaften keinen Kellerbau zulassen?

Gehen wir einem konkreten Verdacht nach, dass fehlende Werte für die *Bsmt*-Variablen mit der Lage der Immobilie zu tun haben. Hierfür können wir die Variable *Neighborhood* und die Variable *Bsmt Cond* verwenden:

```{r}
raw %>% select(Neighborhood, `Bsmt Cond`) %>% 
    mutate(has_basement = !is.na(`Bsmt Cond`)) %>% 
    count(Neighborhood, has_basement) %>% 
    ggplot(aes(x = Neighborhood, y = n, fill = has_basement)) +
    geom_col(position = "fill") + 
    coord_flip() +
    scale_y_continuous(labels = scales::percent)

```

Anscheinend ist der Verdacht eines geographischen Zusammenhangs nicht unbegründet und das hilft uns, die Daten besser zu verstehen.

### Systematisierung der Ursachen fehlender Werte
Missing completely at random (MCAR)
Under MCAR there is no systematic pattern in the location of the missing data points in the dataset: they occur entirely at random. Formally, this means that the probability of an observation being absent depends neither on the values of other variables nor on its own values. In this case, dropping incomplete observations would not introduce bias to the results of the subsequent analysis.
Example: Temperature is continuously measured by a sensor that collects the data and sends them via the Internet to a database. Due to unknown reasons, the Internet connection breaks sometimes.

Missing at random (MAR)
Under MAR, the probability of a particular observation being missing is still independent on its own values, but it does depend on the values of other variables. In this case, removing incomplete observations makes the sample less representative.
Example: Some of the night-hours data are missing due to the sensor's maintenance works, which are always carried out overnight.

Missing not at random (MNAR)
Under MNAR, the probability of an observation being missing depends on its own unobserved values. In this case, again, dropping incomplete data leads to a biased analysis.
Example: Sensor freezes in -20 degrees Celcius and does not measure temperature below this value.


## Ausreisser identifizieren

Die obigen Tabellen bieten konkrete Information und man kann bspw. über den Vergleich des Durchschnitts mit dem Median einen ersten Eindruck der Verteilung der Werte bekommen. Weiterhin wichtig ist die Spalte *outlier*, die anzeigt, wie viele Ausreißer für die jeweilige Variable erkannt wurden. Die Definition, welche Werte als Ausreißer angesehen werden, gleicht der Definition von Ausreißern für *Boxplots* (siehe auch die Dokumentation zur Funktion `boxplot` mittels `?boxplot`).

Tatsächlich ist es sehr hilfreich, Daten und ihre Verteilungen zum besseren Verständnis zu visualisieren. Die Ausreißer der Variable *SalePrice* können wir folgendermaßen untersuchen:

```{r}
plot_outlier(raw, SalePrice)
```

Wie oben erwähnt, können kategorielle Variablen nicht gut numerisch zusammengefasst werden. Die Verteilung der Merkmalsausprägungen ist hingegen sehr wertvoll, weil schnell die häufigen und seltenen Ausprägungen erfasst werden können. Ein einfaches *Histogramm* lässt sich mit dem Paket `ggplot2` folgendermaßen erstellen:
```{r}
library(ggplot2)

raw %>% ggplot(aes(x = `MS Zoning`)) + geom_histogram(stat = "count")
```




Aus der Dokumentation zu dem Paket, die man mittels
```{r}
??AmesHousing
```

aufrufen kann, erfahren wir, dass wir mittels der Funktion `make_ames` eine schon etwas aufbereitete Datenbasis erhalten können:

```{r}
data <- make_ames()
```





## Notizen

Data Doku: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt

GeoLokationen im Artikel http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html









## Colinearity

* Check for colinearity and correlation, because otherwise the choice of reference level will fail

## Reference level for categorical variables

* Choice of reference levels: Modal factor is level 0
* This independent choice of reference levels per variable only works if variables are in fact independent.



