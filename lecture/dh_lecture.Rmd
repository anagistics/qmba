---
title: "Vorlesung Data Science Praxis"
author: 
  - Andreas Cardeneo
date: "`r Sys.Date()`"
output: learnr::tutorial
runtime: shiny_prerendered
bibliography: ["citation.bib"]
biblio-style: "apalike"
link-citations: true
---

<style>
div.boxone {
  padding: 1em;
  background: lightblue;
  color: black;
  border: 1px solid blue;
  border-radius: 5px;
}

div.task {
  padding: 1em;
  background: lightyellow;
  color: black;
  border: 1px solid dimgray;
  border-radius: 5px;
}

div.check {
  padding: 1em;
  background: darkseagreen;
  color: black;
  border: 1px solid darkgreen;
  border-radius: 5px;
}

div.center {
  text-align: center;
}
</style>

```{r setup, include = FALSE}
library(learnr)
tutorial_options(exercise.timelimit = 60, exercise.cap = "Aufgabe")

#write_mode <- TRUE
write_mode <- FALSE
if (!write_mode) {
  dir_pattern <- "www/"
} else {
  dir_pattern <- "www/"
}


suppressPackageStartupMessages({
  library(MASS)
  library(VIM)
  library(dlookr)
  library(mice)
  library(tidyverse)
  library(tidyselect)
  library(patchwork)
  library(corrr)
  library(car)
  library(ggcorrplot)
  library(tidymodels)
  library(glmnet)
  library(ggeffects)
  library(sjPlot)
})

library(AmesHousing)
raw <- ames_raw
names(raw) <- make.names(names(raw))

frage <- purrr::partial(learnr::question, submit_button = "antworten", try_again_button = "nochmal versuchen", allow_retry = TRUE, correct = "Richtig!", incorrect = "Leider falsch!",
                        random_answer_order = TRUE)


```


## Einführung

In dieser Vorlesung geht es um eine Einführung in die praktische Arbeit mit Data Science Methoden. Wir befassen uns anhand eines Szenarios zunächst mit explorativer Datenanalyse, entwickeln dann systematisch ein Modell des maschinellen Lernens und analysieren danach die erhaltenen Ergebnisse. 

Es wird nicht darum gehen, die mathematischen Grundlagen oder sogar Beweise für die Methoden zu erklären: Dafür verweise ich auf die Literatur und andere Vorlesungen. 

Das gezeigte Vorgehen orientiert sich an einem realistischen Szenario und die einzelnen Schritte sind typisch und allgemein auch für andere Szenarios gültig. Wir werden keine *cutting edge*-Methoden verwenden, sondern Lineare Regression - zumindest starten wir damit. Diese Methode ist gut verständlich, theoretisch fundiert und ihre Ergebnisse sind gut *erklärbar*. Das heißt nicht, dass diese Methode auch immer *geeignet* ist, aber in vielen praktischen Fällen kann sie ein guter erster Ansatz sein. Tatsächlich ist die Methode in dem gezeigten Prozess recht einfach austauschbar.

Wir werden einen im Internet und für R verfügbaren Datensatz benutzen, den *Ames Iowa Housing* Datensatz (s. @R-AmesHousing). Dabei handelt es sich um Verkaufsdaten zu Wohnungen und Gebäuden sowie um beschreibende Daten der Immobilien. Es handelt sich damit um ein durchaus anwendungsnahes Szenario.

## Das Szenario

Die *HaiBank AG* möchte im Niedrigzinsumfeld und angesichts steigender Immobilienpreise ihr eigenes Angebot als Immobilienmakler ausbauen. Unter der Marke *ImmoHai* sollen Maklerdienste für Immobilien angeboten werden und eine wichtige Komponente dabei ist die Abschätzung der erwarteten Profitabilität der Vermittlung einer Immobilie. Neben dem eigenen Aufwand auf der Kostenseite ist dafür der erzielbare Verkaufspreis maßgeblich. Traditionell wurde dieser Verkaufspreis von Immobilienexperten nach einer Begehung des Objekts und anhand der Erfahrung eingeschätzt. Weil die Unternehmensstrategie einen deutlichen Ausbau des Maklergeschäfts anstrebt, ist dieser Ansatz aufgrund des hohen Aufwands nicht mehr zu leisten. Stattdessen soll es, ähnlich wie bei der Bonitätsprüfung für Hauskredite, eine Software geben, die anhand von Kennzahlen der Immobilie einen realisierbaren Verkaufspreis schätzt. Diese Software soll von Sachbearbeitern im Kundengespräch verwendet werden und dabei helfen zu entscheiden, ob die *HaiBank* als Makler für die betreffende Immobilie auftreten soll.

### Der Auftrag

<div class="boxone">
<div class="center">
**Auftrag**
</div>

Ihre Aufgabe besteht darin, ein **Vorhersagemodell für Immobilienpreise** und ein technisches Grobkonzept für die *Anwendung*, die dieses Modell verwendet, zu entwickeln.
</div>

### Rahmenbedingungen

Weil Ihre hauseigene IT auf absehbare Zeit noch damit beschäftigt ist, die bislang getrennt in jeder Filiale in Excel-Dateien festgehaltenen Immobiliengeschäfte in eine zentrale Datenbank zu überführen, haben Sie sich aus dem Internet eine frei verfügbare Datensammluung von Immobilienpreisen besorgt. Mit Hilfe dieser Daten können Sie schon einmal den ganzen Prozess der Modellentwicklung durchlaufen und einen Prototypen erstellen, damit Ihr Management auch nachvollziehen kann, wie die fertige Lösung aussehen wird.

Als technische Plattform stehen Ihnen 

* die Programmiersprache *R*, 
* die Entwicklungsumgebung *RStudio* und 
* ggf. das auf *R* basierende Framework *flexdashboard* 

für die Entwicklung von Modell und Anwendung zur Verfügung. *flexdashboard* werden Sie nur verwenden, wenn wir im Rahmen dieses Kurses auch einen Prototypen entwickeln.

#### Grundsätzliches zur Arbeit mit *R*

Neben Python ist *R* eine der derzeit populärsten Sprachen für Datenanalye, Statistik und maschinellem Lernen. Während Python als vielseitige Programmiersprache gerne von einer Community eingesetzt wird, die aus der Softwareentwicklung kommt, hat *R* seinen Ursprung in der statistischen und wissenschaftlichen Community und bietet sehr vielfältige Möglichkeiten für (explorative) Datenanalyse und statistische Methoden. Zu den letzteren kann man im weiteren Sinne auch Methoden des maschinellen Lernens zählen.

##### Einstieg in R

Es gibt zahlreiche Bücher, webbasierte Tutorials und Videos für den Einstieg in R. Dieses Tutorial soll das nicht leisten. Stattdessen verweise ich hier auf eine Reihe von Videos von *Data Science Dojo*, die man hier finden kann: 

![](https://youtu.be/UX6yNAC2sAc)

Die ersten 11 Videos liefern in ca. 45 Minuten einen einfachen Einstieg in die R Syntax, Datentypen und wichtige Sprachkonstrukte.

<div class="task">
<div class="center">
**Aufgabe**
</div>

Gucken Sie sich bitte die ersten 11 Videos an. Die komplette Playlist finden Sie auch unter https://www.youtube.com/playlist?list=PL8eNk_zTBST8j2BU5HYFQogdCjtrHyQAx. Wenn Sie die Videos gesehen haben, werden Sie es bei den folgenden Codebeispielen leichter haben. Wahrscheinlich haben Sie schon spannendere Filme gesehen und vieles kommt Ihnen aus anderen Sprachen bekannt vor - aber R weicht zum Teil deutlich von den üblichen bekannten Sprachen ab, und Sie sollten ein paar der Besonderheiten kennen.

Zeitvorgabe: 45 Min
</div>

## Konzeptionsphase

Sie haben jetzt also den Auftrag erhalten, die oben beschriebene Anwendung auf der Basis eines statistischen Modells zu entwickeln. 

<div class="task">
<div class="center">
**Aufgabe**
</div>

Um ein umfangreiches Verständnis für den Auftrag zu erhalten, sollten Sie die folgenden Punkte mit Ihrem Team besprechen und dokumentieren:

* Wer werden die Anwenderin/der Anwender sein? Wie interagieren diese mit der Applikation? Was erwarten sie?
* Welche Daten werden benötigt? Welchen Lebenszyklus der Daten erwarten Sie, d.h. wie oft ändern sich Daten? Werden Daten entfernt? Wann?
* Werden Sie personenbezogene Daten im Sinne der DSGVO verarbeiten?
* Mit welchen anderen IT-Systemen wird Ihre Applikation kommunizieren?
* Stellen Sie sich das statistische Modell als eine technische Komponente vor, bspw. als Webservice: Wie fügt sich das Modell dann in die Gesamtarchitektur ein?
* Wie ist der gesamte *Fluß* der Applikation? 

Am besten Veranschaulichen Sie das Konzept durch eine einfache Graphik. Dies ist der erste Entwurf und sicher nicht final - streben Sie also keine Perfektion an.

Zeitvorgabe: 15 Min, danach Vorstellung nach Zufallsprinzip.
</div>

### Einordnung des Modells

Es gibt sehr viele verschiedene statische Modelle und Methoden des maschinellen Lernens. Wie lässt sich das angestrebte Modell in diesem Szenario einordnen? 

```{r qregr, echo=FALSE}
frage("Denken Sie an die Ihnen bekannten Typen von Problemstellungen des maschinellen Lernens. Bei der Problemstellung der Immobilienpreisvorhersage handelt es sich um",
  answer("ein Regressionsproblem.", correct = TRUE),
  answer("ein Klassifikationsproblem."),
  answer("ein Clusterungsproblem."),
  answer("ein Zeitreihenprognoseproblem.")
)
```

```{r qclass, echo=FALSE}
frage("Angenommen, das Ergebnis der Software wäre eine Empfehlung zur Annahme oder Ablehnung des Maklerauftrags. Um was für eine Art von Problemstellung würde es dann gehen?",
  answer("Ein Regressionsproblem."),
  answer("Ein Klassifikationsproblem.", correct = TRUE),
  answer("Ein Clusterungsproblem."),
  answer("Ein Zeitreihenprognoseproblem.")
)
```


## Explorative Datenanalyse

Wenn man eine neue Datenquelle erschließen möchte, bestehen die ersten Schritte aus der *explorativen Datenanalye*. Hierbei geht es darum, ein Verständnis für die Daten zu bekommen und insbesondere Fehler und Auffälligkeiten zu entdecken.

Wir werden für die verschiedenen Schritte Gebrauch von mehreren R-Paketen machen. Pakete sind Sammlungen von Funktionen und Daten, die über die Funktion `library` geladen werden:

```{r show_libraries, eval = FALSE}
library(MASS)
library(VIM)
library(dlookr)
library(mice)
library(tidyverse)
library(patchwork)
library(corrr)
library(car)
library(ggcorrplot)
library(tidymodels)
library(glmnet)
library(ggeffects)
library(sjPlot)
```

Wir laden zunächst die Immobiliendaten über das R-Paket `AmesHousing`, weisen den Daten den Namen `raw` zu und korrigieren die Bezeichnungen der Spalten des Datensatzes, so dass diese im weiteren Verlauf gültige R-Bezeichner ergeben:

```{r prep_raw, eval = FALSE}
library(AmesHousing)
raw <- ames_raw
names(raw) <- make.names(names(raw))
```

Die letzte Zeile im obigen Codeblock dient dazu, die Bezeichner der Variablen (Spalten des Datensatzes) so umzuwandeln, dass diese bspw. keine Leerzeichen mehr enthalten, die uns sonst an späterer Stelle stören würden.

Um uns einen ersten Eindruck von den Daten zu verschaffen, benutzen wir die Funktion `glimpse` aus dem `dplyr`-Paket. Dieses Paket ist grundsätzlich eine gute Wahl für die Datenanalyse, weil es zahlreiche recht intuitiv anwendbare Funktionen für Datentransformationen, Datenfilterung und -aggregation bietet. Das Paket `dplyr` gehört zum sog. *tidyverse*, einer Sammlung von Paketen, die ein gemeinsames Konzept hinsichtlich ihrer Syntax und Semantik teilen und in den letzten Jahren die Entwicklung des R-Universums sehr geprägt und gefördert haben. Wir haben das Paket bereits oben geladen.

Doch zurück zu den Daten:

```{r glimpse, exercise = TRUE}
# Lernen Sie die Daten kennen, indem Sie die Funktion `glimpse` auf die Daten `raw` anwenden

```

```{r glimpse-solution}
glimpse(raw)
```



Aus dieser Übersicht erfahren wir schon eine ganze Menge:

* die Zahl der Datensätze
* die Zahl der Merkmale
* die Namen der Merkmale 
* die Datentypen der Merkmale
* eine Vorschau auf die Merksmalsausprägungen.

```{r q_na, echo = FALSE}
frage("Was bedeuten die Angaben 'NA' bei einigen Variablen?",
      answer("Nord-America"),
      answer("Not available", correct = TRUE),
      answer("Not alphanumeric")
)
```

<div class="check">
<div class="center">
**Verständnischeck**
</div>
* Was versteht man unter *kategorialen* Variablen? 
* Was ist der Unterschied zwischen *ordinalen* und *nominalen* Variablen? Kennen Sie Beispiele für beide Typen?
* Was sind *metrische* Variablen? 
* Wie würden Sie Datumwerte einordnen? Was unterscheidet diese von z.B. Preisen?
</div>

### Erstes Verständnis der Daten

Mit `glimpse` konnten wir einen ersten Eindruck der Daten gewinnen. Die zahlreichen Einzelwerte bieten aber noch keinen guten Überblick. Dafür ist eine stärker aggregierte Darstellung besser geeignet. Das Paket `dlookr` (auch bereits geladen) hilft dabei. Angewandt auf unsere Immobiliendaten erhalten wir:
```{r diagnose_all, exercise = TRUE}
# Wenden Sie die Funktion `diagnose` auf die Daten an.

```

```{r diagnose_all-solution}
diagnose(raw)
```
      
        
#### Zusammenfassung numerischer Daten

Wir können zu numerischen Werten mittels der Funktion `diagnose_numeric` mehr Informationen zu den Variablen erhalten, denen ein numerischer Datentyp zu Grunde liegt:

```{r diagnose_num}
diagnose_numeric(raw)
```

#### Zusammenfassung kategorialer Daten

Für kategoriale Variablen lassen sich keine sinnvollen Quantile und auch kein Mittelwert angeben. Hier sieht die Zusammenfassung für die Variable *MS.Zoning* folgendermaßen aus:
```{r diagnose_cat}
diagnose_category(raw, MS.Zoning)
```

*R* bietet übrigens standardmäßig, d.h. ohne weitere Pakete zu laden, die Funktion `summary`, die eine kompakte Übersicht über einen Datensatz bietet:
```{r summary, exercise = TRUE}
# Rufen Sie ein `summary` des Datensatzes auf.

```

```{r summary-solution}
summary(raw)
```

Aber man sieht schon deutlich den Unterschied: Die Ausgabe über die Funktionen des Pakets `dlookr` ist übersichtlicher und auch leichter weiterzuverarbeiten.

### Fehlende Werte

In vielen Datensätzen, die aus unternehmensinternen IT-Systemen bezogen werden, finden sich Datensätze mit Merkmalen ohne Ausprägung. Es gibt keine pauschale Vorgehensweise, die bzgl. der Behandlung dieser fehlenden Werte immer richtig ist. Zunächst kann es sich lohnen die Spalten (Variablen) in den Datensätzen zu identifizieren, die viele fehlende Werte aufweisen.

Gehen wir nochmal zurück zur obigen Diagnose:

```{r diagnose_again}
diagnose(raw)
```

Die Spalte *missing_percent* gibt den Anteil fehlender Werte an und wir können die Spalten mit häufig fehlenden Werten über eine Sortierung erkennen:
```{r diagnose_missing, exercise = TRUE, exercise.lines = 5}
# `diagnose(data) %>% arrange(desc(var))` würde die 
# Tabelle `data` absteigend nach der Variable `var´ sortieren.
# Gehen Sie analog vor, um die Tabelle `raw` wie gewünscht zu sortieren.

```
```{r diagnose_missing-solution}
diagnose(raw) %>% arrange(desc(missing_percent))
```


Deutlich wird, dass die Variable *Pool.QC* wohl häufig keine Werte enthält. Eine Vermutung wäre, dass diese Variable etwas mit dem Vorhandensein eines Pools zu tun hat und dass fehlende Werte schlichtweg bedeuten, dass die Immobilie keinen Pool hat. Etwas anders sieht es bei der Variable *Bsmt.Cond* aus. Diese fehlt in knapp 3% aller Fälle -- ebenso wie andere Variablen mit ähnlichem Namen. Vielleicht ist es hier ähnlich wie bei der Variablen *Pool.QC* und nicht vorhandene Werte bedeuten einen fehlenden Keller. Aber kann es sein, dass bei ca. 3% aller Immobilien kein Keller vorhanden ist? Um dies besser zu verstehen, müsste man mehr über die Bauweise in der Region in Erfahrung bringen: Vielleicht wurden früher bei der Erfassung der Immobiliendaten keine Angaben zum Keller aufgenommen und die Daten fehlen schlichtweg für ältere Immobilien?

```{r q-miss1, echo = FALSE}
frage("Für welche Variable fehlen ca. 2.8% aller Werte?",
      answer("Bsmt.Exposure", correct = TRUE),
      answer("Mas.Vnr.Area"),
      answer("Electrical")
)

```


Unabhängig davon, was hier im konkreten Fall Ursache für die fehlenden Werte ist: Bei der explorativen Datenanalyse müssen wir stets berücksichtigen, dass das Fehlen selbst auch eine fachliche Information (hier: *kein Pool vorhanden*) bedeuten kann und nicht immer auf Fehler bei der Datenaufnahme oder -übertragung zurückzuführen ist bzw. mit Änderungen in der Erfassungssystematik zu tun haben muss.

Wir haben durch die obige Analyse schon den Verdacht, dass fehlende Werte für mehrere Variablen in typischen Kombinationen vorkommen: Wenn es keinen Keller gibt oder keine Werte erfasst wurden, dann fehlen wahrscheinlich Werte für alle diesbezüglichen Variablen. Um das zu untersuchen, können wir das Paket `VIM` verwenden. 

Wenden wir es auf unsere Daten an, wobei wir uns auf die Variablen beschränken, die laut vorangehender Analyse fehlende Werte aufweisen:
```{r missings_chart}
missing_cols <- diagnose(raw) %>% filter(missing_percent > 0.0) %>% pull(variables)

raw %>% select_at(vars(all_of(missing_cols))) %>% aggr(combined = TRUE, sortVars = TRUE, cex.axis = 0.5)
```
<div class="check">
<div class="center">
**Verständnischeck**
</div>
* Wir haben jetzt schon mehrfach den `%>%`-Operator gesehen. Was bedeutet dieser?
* Mit diesem Wissen: Was passiert in der ersten Zeile des vorherigen Codeblocks?
</div>

Die für diese Untersuchung gewählte Beschränkung auf Variablen, die fehlende Werte aufweisen, birgt allerdings ein Problem: Auf diese Weise entgehen uns Zusammenhänge zwischen fehlenden Werten und anderen Variablen, die möglicherweise auf einen systematischen Zusammenhang schließen lassen. Vielleicht fehlen ja die Angaben zu Kellern für Gebäude bestimmter Jahrgänge oder aus bestimmten Gebieten, deren geologischen Eigenschaften keinen Kellerbau zulassen?

Gehen wir einem konkreten Verdacht nach, dass fehlende Werte für die *Bsmt*-Variablen mit der Lage der Immobilie zu tun haben. Hierfür können wir die Variable *Neighborhood* und die Variable *Bsmt.Cond* verwenden:

```{r missings_nhood}
raw %>% select(Neighborhood, Bsmt.Cond) %>% 
    mutate(has_basement = !is.na(Bsmt.Cond)) %>% 
    count(Neighborhood, has_basement) %>% 
    ggplot(aes(x = Neighborhood, y = n, fill = has_basement)) +
    geom_col(position = "fill") + 
    coord_flip() +
    scale_y_continuous(labels = scales::percent)

```

Anscheinend ist der Verdacht eines geographischen Zusammenhangs nicht unbegründet und das hilft uns, die Daten besser zu verstehen.

#### Systematisierung der Ursachen fehlender Werte

In @Sauer2019 (und auch an anderen Stellen) werden die Ursachen für fehlende Werte systematisch beschrieben. Im wesentlichen werden drei verschiedene Fälle unterschieden:

* Missing completely at random (MCAR),
* Missing at random (MAR),
* Missing not at random (MNAR)

Der erste Fall (MCAR) liegt vor, wenn keine bekannte bzw. erkennbare Ursache für das Fehlen vorliegen, d.h. das Fehlen ist insbesondere nicht auf die Ausprägungen anderer Variablen für dieselbe Instanz (dieselbe Beobachtung) zurückzuführen. Typische Ursachen hierfür wären fehlende Daten aufgrund von technischen Übermittlungsfehlern, die nichts mit der Beobachtung selbst zu tun haben. 

Das andere Extrem in dieser Einteilung ist die Klasse MNAR: Hier hängt die Wahrscheinlichkeit für das Fehlen der Ausprägung einer Variablen für eine Beobachtung von der Ausprägung selbst ab. Die Variable *Lot.Frontage* (Bedeutung *Linear feet of street connected to property*) aus unserem Immobiliendatensatz würde hier passen: Die Ausprägung fehlt im Datensatz, sobald die wahre Ausprägung 0 lautet, weil das Grundstück nicht an eine Straße angrenzt.

Dazwischen ist die Klasse MAR. Die Ursache für fehlende Werte liegt hier nicht in der Ausprägung der Variablen selbst, sondern hängt mit anderen Variablen zusammen. Die Variable *Garage.Yr.Blt* (Bedeutung: Baujahr der Garage) fehlt bspw. dann, wenn es keine Garage gibt. Das Fehlen liegt aber nicht am Baujahr selbst.

Die Systematisierung ist kein Selbstzweck. Wenn wir wissen, dass der MNAR-Fall vorliegt, sollten wir über eine neue Variable, die wir selber einführen, unser Wissen über die Gründe für das Fehlen explizit in unseren Datensatz aufnehmen. Im Beispiel könnten wir eine kategoriale Variable *street_access* mit den Werten 0 für *keinen Straßenzugang* und 1 für *mit Straßenzugang* einführen. 
Im Fall von MCAR kann man die fehlenden Werte evtl. gezielt ergänzen. Dieses Vorgehen wird *Imputation* genannt und es gibt verschiedene Verfahren dafür. Andererseits kann man die entsprechenden Instanzen evtl. auch löschen, d.h. für die weitere Betrachtung ignorieren, weil ihr Fehlen keine statistischen Verzerrungen einführt. 
Bei fehlenden Werten nach MAR würde das Löschen von Instanzen allerdings zu statistischen Verzerrungen führen. Im Beispiel hier würde man alle Immobilien ohne Garage entfernen, wenn man die Instanzen löscht, für die die Variable *Garage.Yr.Blt* keine Ausprägung hat. Damit hätte man kein repräsentatives Bild des Immobilienmarkts mehr.

##### Fehlende Werte in der Praxis

In der Praxis, gerade bei der Verwendung von Daten aus operativen IT-Systemen, wird das Fehlen von Werten oftmals durch besondere Ausprägungen codiert. Ein Grund dafür ist, dass die Benutzeroberfläche der IT-Systeme Eingaben in den entsprechenden Feldern erwartet, die der oder die SachbearbeiterIn im Moment der Eingabe nicht leisten kann. In numerischen Feldern wird dann gerne ein Wert wie 99 oder 999 eingetragen, in Textfeldern wird ein Leerzeichen "⍽", ein "-" oder "." eingetragen. 

<div class="check">
<div class="center">
**Verständnischeck**
</div>
In der Finanzbranche sind BI- und Analysesysteme von *SAS* recht verbreitet. 

* Wie werden in SAS fehlender Werte gekennzeichnet? 
* Welche Besonderheit bietet *SAS*?
</div>

#### Umgang mit fehlenden Werten

Wie auch in @Sauer2019 dargestellt wird, ist der richtige Umgang mit fehlenden Werten ein komplexes Thema. Simple Lösungen bestehen darin, die fehlenden Werte durch den Mittelwert der (numerischen) Ausprägungen der Variable zu ersetzen. Bei kategorialen Variablen kann man analog dazu den Modalwert, d.h. die häufigste Ausprägung, als Ersatzwert verwenden. Aber beide Vorgehen sind stark vereinfachend und können zu unrealistischen Merkmalskombinationen für die betroffenen Beobachtungen führen. Technisch kann so eine Ersetzung in R so umgesetzt werden:
```{r tidyr_mean_imputation}
completed <- raw %>% mutate_if(is.numeric, ~ replace_na(., mean(., na.rm = TRUE)))
```

Die Funktion `replace_na` aus dem `tidyr`-Paket ersetzt *NA*-Werte durch den im zweiten Argument angegebenen Wert.

Um andere Imputationsstrategien kennenzulernen, wechseln wir zeitweise zu einem anderen Datensatz:
```{r whiteside_glimpse, exercise = TRUE}
# Verwenden Sie erneut `glimplse`, um sich einen Überblick über den Datensatz `whiteside` zu verschaffen.

```

```{r whiteside_glimpse-solution}
glimpse(whiteside)
```


Der Datensatz passt zum Immobilienszenario, weil er den Zusammenhang zwischen Gasverbrauch, Außentemperatur und Gebäudeisolierung zeigt.

```{r whiteside_functional, exercise = TRUE}
# Erstellen Sie zwei Plots, bei denen Sie jeweils den Gasverbrauch über der 
# Temperatur abtragen und eine Trendgerade zeichnen. Der erste Plot ist 
# für Gebäude vor der Isolierung, der zweite für Messungen danach.
# Anleitung: Ersetzen Sie im Code unten die 'X' durch
# geeignete Variablenbezeichner.
ggplot(whiteside, aes(x = X, y = X)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~ X)
```
```{r whiteside_functional-solution}
ggplot(whiteside, aes(x = Temp, y = Gas)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~ Insul)
```

Für das Ausprobieren entfernen wir zufällig einen kleinen Teil der Messungen des Gasverbrauchs aus den Daten:

```{r whiteside_remove}
set.seed(123)

ws_mod <- whiteside %>% mutate(Gas = if_else(runif(nrow(whiteside)) < 0.15, NA_real_, Gas))
```

<div class="check">
<div class="center">
**Verständnischeck**
</div>
Was bewirkt `set.seed` und warum machen wir das hier?
</div>

Im folgenden probieren wir aus, was passiert, wenn wir die  Imputationsstrategie *Imputation durch Mittelwert* wählen. Dazu verwenden wir das Paket `mice` (@VanBuuren2011).

Die Imputation durch den Mittelwert für alle numerischen Spalten kann durch folgenden Funktionsaufruf erreicht werden:

```{r mice_mean, exercise = TRUE, exercise.lines = 8} 
# Nutzen Sie den `%>%`-Operator, um der Funktion `mice` den Datensatz `ws_mod` zu
# übergeben und setzen Sie die weiteren Parameter dieser Funktion folgendermaßen:
# method = "mean"
# m = 1
# maxit = 1
# Weisen Sie das Ergebnis der Variablen `imp`zu.

```

```{r mice_mean-solution} 
imp <- ws_mod %>% mice(method = "mean", m = 1, maxit = 1)
```

```{r prep_mice_viz_mean_imputation, include = FALSE}
imp <- ws_mod %>% mice(method = "mean", m = 1, maxit = 1)
```

Das Ergebnis der Imputation können wir mit etwas Aufwand visualisieren (das hier geladene Paket `forcats` bietet Funktionen zur Bearbeitung von kategorialen Variablen):
```{r mice_viz_mean_imputation}
tmp1 <- imp$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    arrange(i) %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)

    
```

Aus fachlichen Gründen können wir davon ausgehen, dass es einen Zusammenhang zwischen Temperatur und Gasverbrauch gibt: Die Trendlinien oben legen das auch nahe.

```{r mice_lm_imputation, exercise = TRUE, exercise.lines = 10}
# Gehen Sie analog zu oben vor und weisen Sie der Variablen `imp2` das Ergebnis
# des Funktionsaufrufs von `mice` auf den Daten `ws_mod` zu. Wählen Sie die folgenden
# weiteren Parameter für `mice`:
# method = "norm.predict"
# seed = 1
# m = 1
# print = FALSE

```

```{r mice_lm_imputation-solution}
imp2 <- ws_mod %>% mice(method = "norm.predict", seed = 1, m = 1, print = FALSE)
```


```{r prep_mice_lm_imputation, include = FALSE}
imp2 <- ws_mod %>% mice(method = "norm.predict", seed = 1, m = 1, print = FALSE)
```

Das Ergebnis dieser Imputation können wir auch wieder mit dem obigen Code visualisieren:
```{r mice_viz_lm_imputation}
tmp1 <- imp2$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)
```

Dieses Ergebnis sieht wesentlich plausibler aus als der erste Versuch auf Basis des Mittelwerts. 


### Ausreisser identifizieren

Wir kehren zurück zum *AmesHousing*-Datensatz.

Die obigen Diagnosetabellen bieten konkrete Information und man kann bspw. über den Vergleich des Durchschnitts mit dem Median einen ersten Eindruck der Verteilung der Werte bekommen. Weiterhin wichtig ist die Spalte *outlier*, die anzeigt, wie viele Ausreißer für die jeweilige Variable erkannt wurden. Die Definition, welche Werte als Ausreißer angesehen werden, gleicht der Definition von Ausreißern für *Boxplots* (siehe auch die Dokumentation zur Funktion `boxplot` mittels `?boxplot`).

Tatsächlich ist es sehr hilfreich, Daten und ihre Verteilungen zum besseren Verständnis zu visualisieren. Die Ausreißer der Variable *SalePrice* können wir folgendermaßen untersuchen:

```{r plot_outliers}
plot_outlier(raw, SalePrice)
```

Prüfen Sie selbst die Ausreisser für andere Variablen, bspw. *X2nd.Flr.SF* (die Fläche der 2. Etage in Quadratfuß):
```{r plot_outliers_self, exercise = TRUE}

```

```{r plot_outliers_self-solution}
plot_outlier(raw, X2nd.Flr.SF)
```


#### Ausreisser behandeln

Wenn man feststellt, dass es sich bei den Ausreissern nicht um Fehler handelt, bspw. Tippfehler oder falsche Sensormeldungen, hat man folgende Möglichkeiten, mit den Ausreissern umzugehen:
* *Trimmen*: Beim Trimmen gibt man einen Wert $p\in[0,1]$ vor, bspw. $p = 0.9$, und schließt dann alle Beobachtungen aus, bei denen die betrachtete Variable einen Wert unterhalb des $100\cdot\frac{p}{2}\%$-Perzentils bzw. oberhalb des $100\cdot(1 - \frac{p}{2})\%$-Perzentils hat. Im Beispiel würde man also alle Beobachtungen mit Werten unterhalb des 5%- bzw. oberhalb des 95%-Perzentils entfernen.
* *Winsorisieren*: Hier gibt man auch einen Wert $p\in[0,1]$ vor, bspw. $p = 0.9$, und setzt dann alle Datenwerte unterhalb des $100\cdot\frac{p}{2}\%$-Perzentils auf den Wert des $100\cdot\frac{p}{2}\%$-Perzentils und analog dazu alle Werte größer als das $100\cdot(1 - \frac{p}{2})\%$-Perzentil auf den Wert des $100\cdot(1 - \frac{p}{2})\%$-Perzentils. Im Beispiel wären die untere bzw. obere Grenze also das 5%- bzw. das 95%-Perzentil.

Beim Winsorisieren verliert man also keine Datensätze, verändert den vorhandenen aber. 

### Seltene Werte

Wie oben erwähnt, können kategoriale Variablen nicht gut numerisch zusammengefasst werden. Die Verteilung der Merkmalsausprägungen ist hingegen sehr wertvoll, weil schnell die häufigen und seltenen Ausprägungen erfasst werden können. Ein einfaches *Histogramm* für die Variable *MS.Zoning* lässt sich mit dem Paket `ggplot2` folgendermaßen erstellen:
```{r example_rare_values}
raw %>% ggplot(aes(x = MS.Zoning)) + geom_histogram(stat = "count")
```

Erstellen Sie selbst analog ein Histogramm für die Variable *Overall.Cond*:
```{r self_histo, exercise = TRUE}

```

```{r self_histo-solution}
raw %>% ggplot(aes(x = Overall.Cond)) + geom_histogram(stat = "count")
```




```{r prep_var_type_handling, include = FALSE}
housing_variable_types <- function(doc_url = "http://jse.amstat.org/v19n3/decock/DataDocumentation.txt") {
  doku <- url(doc_url, encoding = "latin1") %>% readLines() 
  data_type_lines <- grep("Nominal|Ordinal|Discrete|Continuous", doku, value = TRUE)

  data_type_lines %>% str_match_all("^(.+)( |\\s)\\((.+)\\)(\\s*):") %>% compact %>% 
    map(~ c(VAR = .[,2], TYPE = .[,4])) %>% map_dfr(as.list) %>% 
    mutate(VAR = make.names(VAR))
}

type_data <- housing_variable_types()

housing_categorials <- function(type_frame) {
  type_frame %>% filter(TYPE %in% c("Nominal", "Ordinal")) %>% select(VAR) %>% 
    mutate(VAR = case_when(
        VAR == "Exterior.1" ~ "Exterior.1st",
        VAR == "Exterior.2" ~ "Exterior.2nd",
        VAR == "HeatingQC" ~ "Heating.QC",
        VAR == "KitchenQual" ~ "Kitchen.Qual",
        VAR == "FireplaceQu" ~ "Fireplace.Qu",
        VAR == "BsmtFinType.2" ~ "BsmtFin.Type.2",
        TRUE ~ VAR
    )) %>% pull()
}

categorials <- housing_categorials(type_data)

raw <- raw %>% mutate_at(vars(all_of(categorials)), as.factor) 
```


Aus Histogrammen können wir ableiten, welche Werte je Variable sehr selten auftreten. Gerade bei der Analyse kategorialer Variablen stellt sich oft heraus, dass einige Ausprägungen sehr selten auftreten. Eine Analyse auf Basis dieser Ausprägungen würde statistisch wertlos sein, da die Anzahl der Beobachtungen pro Ausprägung nicht für statistisch valide Aussagen ausreicht. Daher ist es empfehlenswert und üblich, seltene Ausprägungen zu Gruppen zusammenzufassen und eine neue Dummy-Ausprägung, bspw. *sonstige*, für diese einzuführen.

Im Folgenden untersuchen wir die kategorialen Variablen des *AmesHousing*-Datensatzes. Um eine vollständige Liste der kategorialen Variablen zu erhalten, hilft die [Dokumentation des AmesHousing-Datensatzes](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). Übrigens finden sich auf der Seite [Gimme Shelter](https://www.gimme-shelter.com/category/term-2/) recht gute Erklärungen einiger (amerikanischen) Fachbegriffe der Immobilienbranche. 

In der Datendokumentation sehen wir, dass kategoriale Variablen als *Nominal* oder *Ordinal* aufgeführt werden. Wir könnten jetzt manuell eine Liste anlegen, aber als Data Scientists machen wir das natürlich nicht, sondern extrahieren die notwendige Information als Datensatz (die Basis für den hier verwendeten Code kommt von [Ron Sarafian](http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html)).

```{r derive_types_from_doc, eval = FALSE}
housing_variable_types <- function(doc_url = "http://jse.amstat.org/v19n3/decock/DataDocumentation.txt") {
  doku <- url(doc_url, encoding = "latin1") %>% readLines() 
  data_type_lines <- grep("Nominal|Ordinal|Discrete|Continuous", doku, value = TRUE)

  data_type_lines %>% str_match_all("^(.+)( |\\s)\\((.+)\\)(\\s*):") %>% compact %>% 
    map(~ c(VAR = .[,2], TYPE = .[,4])) %>% map_dfr(as.list) %>% 
    mutate(VAR = make.names(VAR))
}

type_data <- housing_variable_types()
```

Jetzt liegen die Spaltenbezeichnungen zusammen mit der Datenart als `data.frame` für die weitere Verarbeitung vor. Leider stellt sich heraus, dass die Dokumentation in einigen Fällen von den Spaltenbezeichnung im Datensatz abweicht, so dass wir diese Bezeichnungen manuell korrigieren müssen. Ein schönes Beispiel dafür, dass man bei der explorativen Datenanalyse immer wieder unerwartet auf Probleme stößt.

```{r fix_broken_docs, eval = FALSE}
housing_categorials <- function(type_frame) {
  type_frame %>% filter(TYPE %in% c("Nominal", "Ordinal")) %>% select(VAR) %>% 
    mutate(VAR = case_when(
        VAR == "Exterior.1" ~ "Exterior.1st",
        VAR == "Exterior.2" ~ "Exterior.2nd",
        VAR == "HeatingQC" ~ "Heating.QC",
        VAR == "KitchenQual" ~ "Kitchen.Qual",
        VAR == "FireplaceQu" ~ "Fireplace.Qu",
        VAR == "BsmtFinType.2" ~ "BsmtFin.Type.2",
        TRUE ~ VAR
    )) %>% pull()
}

categorials <- housing_categorials(type_data)

```

Zunächst modifizieren wir unseren Datensatz so, dass R die kategorialen Variablen erkennt, indem wir die entsprechenden Spalten zu `factors` konvertieren.
```{r make_factors, eval = FALSE}
raw <- raw %>% mutate_at(vars(all_of(categorials)), as.factor) 
```


Jetzt können wir uns ansehen, wieviele verschiedene Ausprägungen wir für die kategorialen Variablen haben. Nachdem wir schon wissen, dass *PID* eine eindeutige Kennung ist, lassen wir diese Variable aus der Betrachtung heraus. Für die graphische Darstellung bringen wir die Daten mit der `pivot_longer`-Funktion in ein *langes Format* (s. auch @Sauer2019), bei dem die Variablen nicht mehr einzeln in Spalten stehen. Zum besseren Verständis wird die Bildung des langen Datensatzes auf zwei Schritte mit zwischenzeitlicher Ausgabe der Tabelle aufgeteilt.

```{r count_unique_categories, fig.height = 15} 
categ_freq <- raw %>% select(all_of(categorials)) %>% select(-PID) %>% 
    summarise_all(~ length(unique(.))) 

head(categ_freq)

categ_freq <- categ_freq %>% 
  pivot_longer(cols = everything(), names_to = "Variable", 
               values_to = "Werte")

head(categ_freq)

categ_freq %>% 
    ggplot(aes(x = reorder(Variable, Werte), y = Werte)) + geom_col() + coord_flip()
```

Im nächsten Schritt gucken wir uns alle kategorialen Variablen an, die 6 oder mehr verschiedene Ausprägungen haben. Für diese erstellen wir jeweils ein Histogramm der Ausprägungen, um zu sehen, ob es seltene Ausprägungen gibt.
```{r find_rare_level_factors}
rel_categs <- categ_freq %>% filter(Werte >= 6) %>% pull(Variable)

categ_levels <- raw %>% select(all_of(rel_categs)) %>% map(~ fct_count(.))

```

Die Informationen über die Häufigkeit visualisieren wir: Weil alle unsere Plots verschiedene Ausprägungen haben und unterschiedlich viele Ausprägungen, bietet sich der üblich Weg in `ggplot` über `facet_wrap` oder `facet_grid` nicht an. Hier ist das Paket `patchwork` hilfreich, durch das wir eine Liste von Plots automatisch in einer Graphik anordnen können.
```{r viz_rare_factor_levels, fig.height = 24}
plot_level_freq <- function(idx, data) {
    data[[idx]] %>% ggplot(aes(x = reorder(f, n), y = n)) + geom_col() + 
    coord_flip() + 
    xlab(names(data)[[idx]])
}

var_cl_plots <- lapply(seq_along(categ_levels), plot_level_freq, categ_levels)

wrap_plots(var_cl_plots, ncol = 2)
```

Zwar sind nicht alle Plots gut lesbar, aber wir sehen, was wir wissen wollten: Es gibt für viele kategoriale Variablen Gruppen von Ausprägungen, die sehr selten vorkommen.
Weiter unten bei der Datenvorverarbeitung für die Modellierung, werden wir daher solche kategorialen Variablen entsprechend bearbeiten und seltene Ausprägungen zusammenfassen.

### Fachliche Transformationen

Manchmal ist es sinnvoll, Variablen aus fachlichen Gründen zu transformieren. Hier am Beispiel von Immobilien können wir uns vorstellen, dass der Preis der Immobilie von deren Alter abhängt. Die Variable *Year.Built* erfasst das Baujahr und *Year.Remod.Add* erfasst das Jahr des letzten Umgestaltung. Wir könnten hier zwei neue Variablen einführen, die stattdessen das *Alter* der Immobilie bzw. die Jahre seit der letzten Umgestaltung abbilden. 
Die Dokumentation des Datensatzes zeigt auch, dass sich bspw. die Gesamtzahl der Badezimmer erst durch das Zusammenführen mehrerer Variablen ergibt. Es könnte sinnvoll sein, auch hierfür eine neue Variable anzulegen.

Wir kommen im Abschnitt *Datenvorverarbeitung* darauf zurück.

### Informationsbeiträge von Variablen

Nachdem wir uns im vorherigen Abschnitt mit kategorialen Variablen befasst haben, kehren wir hier wieder zurück zu den numerischen Variablen. Wir sollten uns in Erinnerung rufen, dass wir letztlich ein Modell für den Verkaufspreis von Immobilien entwickeln wollen. Dahinter steckt die Annahme, dass verschiedene Eigenschaften bzw. Eigenschaftskombinationen von Immobilien Einfluss auf den Preis haben. 

#### Beitrag einzelner Variablen

Wir können davon ausgehen, dass Eigenschaften, die für alle Immobilien gleich oder zumindest sehr ähnlich sind, auf den Preis keinen Einfluss haben werden. Wenn wir also Variablen identifizieren, die für viele oder alle Beobachtungen gleich sind, können wir diese von der weiteren Betrachtung ausschließen.

Wenn wir nur einzelne Variablen betrachten, können wir als Statistik für den Informationsgehalt numerischer Variablen die Varianz der Ausprägungen (Werte) verwenden. Die Spalte *Order* können wir dabei ignorieren, weil sie einfach nur eine Ordnungszahl enthält.

Als ersten Schritt visualisieren wir die Verteilungen (genauer: Dichtefunktionen) der numerischen Variablen. Nachdem wir oben schon die kategorialen Variablen identifiziert haben, können wir die numerischen einfach als Differenz zur vollständigen Variablenmenge ermitteln:

```{r define_numeric_columns}
numeric_cols <- setdiff(names(raw), categorials)
```

Für die graphische Darstellung müssen wir den Datensatz wieder in ein *langes Format* bringen, analog dem Vorgehen weiter oben. 
```{r viz_numeric_column_densities, fig.height=24}
raw_rs <- raw %>% select_at(all_of(numeric_cols)) %>% select(-Order) %>% 
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Wert") 

plot_density <- function(data) {
    data %>% filter(!is.na(Wert)) %>% 
        ggplot(aes(x = Wert)) + geom_density() + xlab(first(data$Variable))
}

var_data <- raw_rs %>% group_split(Variable)

plots <- lapply(var_data, plot_density)

wrap_plots(plots, ncol = 2)
```

<div class="check">
<div class="center">
**Verständnischeck**
</div>
* Was bewirkt `raw_rs %>% group_split(Variable)` in obigem Code?
* Welchem Konstrukt in Python entspricht `lapply(var_data, plot_density)`?
</div>


Durch die Dichtediagramme können wir schon ahnen, welche Variablen wahrscheinlich eine niedrige Varianz aufweisen. Allerdings kann die Intuition auch täuschen, so dass wir die Varianzen der Variablen auch noch rechnerisch bestimmen:
```{r compute_variance}
(nvar_var <- raw %>% select_at(all_of(numeric_cols)) %>% select(-Order) %>% 
    summarise_all(~ var(., na.rm = TRUE) ) %>%     
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Varianz") %>% 
        arrange(Varianz))
```

Für die Datenvorverarbeitung werden wir Variablen entfernen, die eine niedrige Varianz haben. Umgekehrt können wir in der Analyse auch die Variablen ignorieren, die nur verschiedene Werte aufweisen - hier dürfte es sich fast immer um Datensatzschlüssel handeln.

#### Zusammenhang zwischen Variablen: Korrelation

Zwischen metrischen Variablen bestehen oftmals mehr oder weniger stark ausgeprägte lineare Zusammenhänge. Dieser Zusammenhang lässt sich als *Korrelationskoeffizient* (man unterscheidet mehrere verschiedene Korrelationskoeffizienten, s. auch @Sauer2019. Hier verwenden wir den nach *Pearson*). Der Begriff *metrische Variable* bedeutet, dass die Variable intervall- oder verhältnisskaliert ist. Das gilt für stetige numerische Variablen und oftmals auch für diskrete numerische Variablen - im Prinzip immer dann, wenn man die Variable messen oder zumindest zählen kann. 

Der Korrelationskoeffizient $Corr(X, Y)$ zwischen den Variablen $X$ und $Y$ ist definiert als 

$$ Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X\cdot\sigma_Y} \in [-1, 1], $$
wobei $Cov(X, Y)$ die *Kovarianz* zwischen den Variablen ist. Eine anschauliche Beschreibung der Korrelation findet sich in @Sauer2019 und in nahezu jedem Statistiklehrbuch.

In R gibt es mehrere Möglichkeiten der Berechnung des Korrelationskoeffizienten nach Pearson, u.a. die Standardfunktion `cor` und die hier verwendete Funktion `correlate` aus dem `corrr`-Paket:

```{r prep_corr_mat, include = FALSE}
corr_mat <- raw %>% select(Gr.Liv.Area, Lot.Area) %>% correlate()
```

```{r example_correlation, exercise = TRUE, exercise.lines = 10}
# Für einen Datensatz `data` mit den Variablen `A` und `B`, können wir die Korrelation durch das Statement
# data %>% select(A, B) %>% correlate()
# berechnen.
# Weisen Sie der Variablen `corr_mat` die Korrelation von `Gr.Liv.Area` und `Lot.Area` aus dem Datensatz `raw` zu.
# Zeigen Sie die Variable anschließend an.

```

```{r example_correlation-solution}
corr_mat <- raw %>% select(Gr.Liv.Area, Lot.Area) %>% correlate()
corr_mat
```

Wenn wir die Korrelation zwischen vielen Variablenpaaren auswerten wollen, bietet das Paket `ggcorrplot` schöne Unterstützung:
```{r viz_correlations, fig.width = 10}
corr_mat <- raw %>% select_at(all_of(numeric_cols)) %>% 
    select(-Order) %>% drop_na() %>% 
    cor()

corr_mat %>% ggcorrplot(type = "upper", insig = "blank", 
                        hc.order = TRUE, 
                        tl.cex = 8, tl.srt = 60)

```

Zu beachten ist hier, dass die Beobachtungen mit fehlenden Werten entfernt werden mussten (mittels `drop_na`), da ansonsten keine Korrelation hätte berechnet werden können. 

Wir können anhand des Diagramms auch schon erkennen, dass es einige (aber nicht viele) Variablen gibt, die stärker (positiv) mit dem Verkaufspreis korreliert sind.

#### Multikollinearität

Allgemein spricht man von *(Multi)Kollinearität*, wenn zwei oder mehrere unabhängige Variablen stark miteinander korreliert sind (s. @Sheather2009). In verschiedenen Methoden, darunter auch linearer Regression, führt Multikollinearität dazu, dass die Bedeutung einzelner Variablen nicht isoliert bewertet werden kann. Es kann sogar passieren, dass die Regressionskoeffizienten das falsche Vorzeichen haben und damit nicht nachvollziehbare Abhängigkeiten suggerieren (s. @Sheather2009 für ein Beispiel). 

<div class="check">
<div class="center">
**Verständnischeck**
</div>
Kurz zur Wiederholung: Wir sprechen bei (multipler) linearer Regression über das Modell
$$ y = \beta_0 + \sum_{i=1}^n \beta_i\cdot x_i + \varepsilon.$$

* Wenn im obigen Abschnitt von *Regressionskoeffizienten* die Rede ist: Wo finden sich diese im Modell?
* Entsprechend: Was wird in diesem Zusammenhang als *Variable* bezeichnet?
</div>

Um zu ermitteln, wie stark Variablen von Multikollinearität betroffen sind, wird der sog. *Variance Inflaction Factor* (VIF) bestimmt. Die Bezeichung kommt daher, dass man zeigen kann (s. @Sheather2009), dass die Varianz der Regressionskoeffizienten einer Variablen mit der Korrelation zu anderen (unabhängigen) Variablen steigt. Da eine große Varianz der Regressionskoeffizienten für ein valides Modell unerwünscht ist, kann man dann entscheiden, Variablen mit großen VIF nicht in das Modell aufzunehmen.  

In R gibt es (natürlich) verschiedene Möglichkeiten, den VIF-Wert zu bestimmen. Hier verwenden wir die `vif`-Funktion aus dem `car`-Paket. 
Die Funktion erhält als Eingabe ein *lineares Modell* basierend auf einer R *Formel*, die die abhängige Variable zu den unabhängigen in Beziehung setzt. Wir probieren das an dieser Stelle für einige ausgewählte Variablen aus (und orientieren uns dabei an der Graphik zur Korrelation oben).
Zunächst erstellen wir ein lineares Modell mit dem Verkaufspreis al abhängiger Variable und mehreren unabhängigen.

```{r example_linear_model}
linmod <- lm(SalePrice ~ Gr.Liv.Area + Wood.Deck.SF + Bedroom.AbvGr + 
               Full.Bath + TotRms.AbvGrd,
             data = raw %>% select_at(all_of(numeric_cols)))

summary(linmod)

```

Der Koeffizient für die Variable *Bedroom.AbvGr* ist negativ, was erstmal erstaunlich ist, denn das würde bedeuten, dass der Verkaufspreis mit mehr Schlafzimmern sinkt. Gucken wir uns jetzt die *VIF* an:
```{r compute_vif}
vif(linmod)
```

Der VIF-Wert für eine unabhängige Variable wird über das *Bestimmtheitsmaß* $R^2$ des linearen Regressionsmodells auf den verbleibenden unabhängigen Variablen berechnet: 

```{r lm_for_indep_bedroom}
bdrm_mod <- lm(formula = Bedroom.AbvGr ~ Gr.Liv.Area + Wood.Deck.SF + 
                 Full.Bath + TotRms.AbvGrd, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(bdrm_mod)
```

Der VIF-Wert für die Variable *Bedroom.AbvGr* wird dann berechnet als

$$ VIF(\text{Bedroom.AbvGr}) = \frac{1}{1 - R_{\text{Bedroom.AbvGr}}^2} = \frac{1}{1 - 0.4597} = 1.851.$$

Analog wird der VIF-Wert für die Variable *Gr.Liv.Area* ermittelt:

```{r lm_for_indep_livarea, exercise = TRUE, exercise.lines = 10}
# Erstellen Sie analog zum letzten Codeblock ein lineares Modell für die Variable `Gr.Liv.Area` 
# in Abhängigkeit der anderen vier Variablen und weisen Sie es der Variablen `gla_mod' zu.
# Lassen Sie sich ein `summary` des Modells ausgeben.

```

```{r lm_for_indep_livarea-solution}
gla_mod <- lm(formula = Gr.Liv.Area ~ Bedroom.AbvGr + Wood.Deck.SF + Full.Bath + TotRms.AbvGrd, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(gla_mod)
```

```{r prep_lm_for_indep_livarea, include = FALSE}
gla_mod <- lm(formula = Gr.Liv.Area ~ Bedroom.AbvGr + Wood.Deck.SF + Full.Bath + TotRms.AbvGrd, 
               data = raw %>% select_at(all_of(numeric_cols)))

```


$$ VIF(\text{Gr.Liv.Area}) = \frac{1}{1 - R_{\text{Gr.Liv.Area}}^2} = \frac{1}{1 - 0.7205} = 3.578.$$

Die Frage ist nun, ob solche VIF-Werte bedenklich sind. Diese Frage ist nich so einfach zu beantworten. Klar ist, dass der VIF-Wert mindestens 1 beträgt und gegen unendlich gehen kann. Bei einem Bestimmtheitsmaß $R^2$ von 0.64, d.h. einem *Korrelationskoeffizienten* von 0.8 (also schon recht stark korreliert), würde man einen VIF-Wert 
$$ VIF = \frac{1}{1 - 0.64} = 2.78 $$ erhalten. In der Literatur (u.a. auch in @Sheather2009), wird oft ein VIF von mindestens 5 als kritisch gesehen. Das würde einem (betragsmäßigen) Korrelationskoeffizienten von 
$$ R = \sqrt{1 - \frac{1}{VIF}} = \sqrt{1 - \frac{1}{5}} = 0.89$$ 
entsprechen, also knapp 0.9. Wir sehen, der VIF-Wert wächst recht schnell für kleinere Änderungen des Korrelationskoeffizienten und es gibt auch Quellen, z.B. https://statisticalhorizons.com/multicollinearity, die ab einem VIF von 2.5 vorsichtig werden. 

Wie würde das Regressionsmodell aussehen, wenn wir nur die hochkorrelierte Variable *Gr.Liv.Area* verwenden würden?
```{r example_simple_linear_model}
gla1_mod <- lm(formula = SalePrice ~ Gr.Liv.Area, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(gla1_mod)
```

Und wenn wir die anderen nicht hoch korrelierten Variablen nehmen?

```{r example_three_var_linear_model}
lc_mod <- lm(formula = SalePrice ~ Wood.Deck.SF + Bedroom.AbvGr + Full.Bath, 
             data = raw %>% select_at(all_of(numeric_cols)))

summary(lc_mod)
```

Also offensichtlich können wir nicht einfach alle hochkorrelierten Variablen entfernen, ohne die Modellqualität (hier gemessen anhand des Bestimmtheitsmaßes) deutlich zu verschlechtern. Der VIF-Wert gibt uns leider nicht an, woher genau die hohe Korrelation stammt, aber aus der obigen Korrelationsmatrix können wir sehen, dass *TotRms.AbvGrd* stark mit den anderen Variablen korreliert ist. Daher prüfen wir ein Modell ohne diese Variable:

```{r example_four_var_linear_model}
gwbf_mod <- lm(formula = SalePrice ~ Gr.Liv.Area + Wood.Deck.SF + Bedroom.AbvGr + Full.Bath, 
             data = raw %>% select_at(all_of(numeric_cols)))

summary(gwbf_mod)

vif(gwbf_mod)
```

Damit erreichen wir die Modellqualität des ersten Modells und senken die VIF-Werte, d.h. wir haben ein einfacheres Modell gleicher Qualität erreicht, das besser erklärbar und robuster ist (weniger Varianz in den Regressionskoeffizienten). Aber der Koeffizient für die Schlafzimmerzahl ist immer noch negativ...

## Datenvorverarbeitung

Nachdem wir ein besseres Verständnis für die Daten und die Zusammenhänge zwischen den Variablen erhalten haben, können und müssen wir vor der Modellerstellung noch weitere Vorverarbeitungsschritte vornehmen. Tatsächlich kann dieser Schritt nicht ohne Verständnis für das angestrebte Modell durchgeführt werden: Einige Arten von Modellen können bspw. nicht mit fehlenden Werten oder kategorialen Variablen umgehen (beides gilt für lineare und logistische Regression - hier darf man aber nicht das Verfahren an sich mit der Implementierung des Verfahrens verwechseln. Diese kann bspw. automatisch Datensätze mit fehlenden Werten entfernen und dann wirkt das Verfahren nach außen hin so, als könnte es mit fehlenden Werten umgehen). Nachdem man zu Beginn oftmals noch nicht sicher weiß, welche Art von Modell das beste Ergebnis liefert, ist die Datenvorbereitung in der Praxis ein iterativer Prozess. 

Wir werden uns, wie oben auch schon praktiziert, hier mit einem linearen Regressionsmodell beschäftigen. Unser Modell wird also die Form
$$ \hat{y} = \hat{\beta}_0 + \sum_{i=1}^n \hat{\beta}_i\cdot x_i + \varepsilon$$ haben.
Dabei ist $\hat{y}$ die Schätzung für die *Zielvariable* (hier der Verkaufspreis), die $x_i$ sind $n$ unabhängige Variablen (die Prädiktoren) und $\varepsilon$ ist ein Fehlerterm. 
Gesucht sind die Koeffizienten $\hat{\beta}_0$ (konstanter Term, *intercept*) und $\hat{\beta}_i$. Ermittelt werden diese Koeffizienten anhand von $N$ *Trainingsdatensätzen* $(y^{(j)}, x_1^{(j)}, x_2^{(j)},\ldots,x_n^{(j)}), j = 1,\ldots,N$ durch Lösen des Optimierungsproblems 
$$ \min_{\hat{\beta}_0, \hat{\beta}_1,\ldots,\hat{\beta}_n} \sum_{j=1}^N \left(y^{(j)} - \left(\hat{\beta}_0 + \sum_{i=1}^n \hat{\beta}_i\cdot x_i^{(j)}\right)\right)^2. $$
Für dieses Optimierungsproblem kann man geschlossene Ausdrücke für die optimalen Koeffizienten $\beta_0^*, \beta_1^*,\ldots, \beta_n^*$ angeben und die Lösung somit effizient berechnen (s. @Sheather2009).

Für die Datenvorbereitung des Modells werden wir das `recipes`-Paket verwenden. Dieses Paket bietet schon eine Vielzahl typischer Vorbereitungsschritte, die sich einfach zu einer *Pipeline* zusammensetzen lassen. Diese Pipeline kann dann auch einfach wieder auf andere Daten angewandt werden, was die Reproduzierbarkeit und Wiederverwendung erleichtert.

Wir beginnen das *Rezept* mit der Aufteilung des Datensatzes in einen Trainings- und einen Testdatensatz. Auf Basis des Trainingsdatensatzes wird das Modell *gelernt* und die Güte des gelernten Modells wird dann anhand des Testdatensatzes gemessen. Damit ist sichergestellt, dass wir messen, wie gut das Modell die wesentlichen Eigenschaften der Daten abbildet und generalisieren kann: Schließlich sind wir daran interessiert, eine Verkaufspreisvorhersage für zuvor ungesehene Daten zu erhalten.

Damit wir aber sicher sind, dass wir auch auf den richtigen Daten arbeiten, wiederholen wir an dieser Stelle ein paar der Erstellungsschritte von oben:

### Grundlage (wieder)herstellen

```{r recreate_housing_data}
housing_data <- ames_raw
names(housing_data) <- make.names(names(housing_data))

type_data <- housing_variable_types()
categorials <- housing_categorials(type_data)

housing_data <- housing_data %>% mutate_at(vars(all_of(categorials)), as.factor) %>% 
  select(-Order, -PID)

```

```{r prep_housing, include = FALSE}
housing_data <- housing_data %>% 
  mutate(Age = max(Year.Built) - Year.Built, 
         Years_since_remod = max(Year.Remod.Add) - Year.Remod.Add) %>% 
  select(-Year.Built, -Year.Remod.Add)

set.seed(123456)

data_split <- initial_split(housing_data)

training_data <- training(data_split)
test_data <- testing(data_split)

housing_rec <- recipe(SalePrice ~ ., data = training_data)
```


### Transformationen
In diesem Abschnitt führen wir die zuvor genannten fachlichen Transformationen durch:
```{r domain_specific_transformations, eval = FALSE}
housing_data <- housing_data %>% 
  mutate(Age = max(Year.Built) - Year.Built, 
         Years_since_remod = max(Year.Remod.Add) - Year.Remod.Add) %>% 
  select(-Year.Built, -Year.Remod.Add)
```


### Trainings- und Testdatensplit
```{r create_data_split, eval = FALSE}
set.seed(123456)

data_split <- initial_split(housing_data)

training_data <- training(data_split)
test_data <- testing(data_split)
```

```{r q-trng_tst_split, echo = FALSE}
frage("Indem die Funktion `initial_split` verwendet wird, werden die Daten in 
      Trainings- und eine Testmenge unterteilt. In welchem Verhältnis?",
      answer("Das Verhältnis ist zufällig. Deswegen auch der Aufruf von `set.seed`"),
      answer("Im Verhältnis 1:1, beide Mengen sind danach gleich groß"),
      answer("Im Verhältnis 80:20 nach dem Pareto-Prinzip"),
      answer("Im Verhältnis 75:25, d.h. die Trainingsmenge ist 3mal so groß wie die Testmenge", correct = TRUE)
)

```


### Rezept anlegen
Das Rezept wird angelegt, indem wir die Zielvariable angeben und pauschal alle anderen Variablen als potentielle Prädiktoren vorsehen. Im weiteren Verlauf werden wir das Rezept durch die Verkettung mit `step_*`-Funktionen weiter spezifizieren. Viele der `step_*`-Funktionen aus dem `recipes`-Paket bieten noch Parameter, um das Verhalten weiter zu beeinflussen. Die Dokumentation zum `recipes`-Paket listet unter https://recipes.tidymodels.org/articles/Ordering.html auch eine empfohlene Reihenfolge der Schritte auf.

```{r create_recipe, eval = FALSE}
housing_rec <- recipe(SalePrice ~ ., data = training_data)
```

### Imputation fehlender Werte
Zunächst werden wir fehlende Werte mit einer komplexeren Imputationsstrategie, nämlich *kNN-Imputation* (s. u.a. @Beretta2016), durch plausible Werte ersetzen:
```{r recipe_step_imputation, eval = FALSE}
housing_rec %>% 
  step_knnimpute(all_predictors(), neighbors = 3) -> housing_rec
```

Um noch etwas deutlicher den *Fluss* der Transformationen darzustellen, haben wir hier den Zuweisungsoperator in der Form `->` verwendet.


### Seltene Kategorien zusammenfassen

Wir haben oben gesehen, dass es einige kategoriale Variablen gibt, die sehr seltene Ausprägungen aufweisen. Diese fassen wir zu Gruppen zusammen:
```{r recipe_step_rare_levels, eval = FALSE}
housing_rec %>% 
  step_other(all_nominal(), threshold = 0.05) -> housing_rec
```

### Codierung kategorialer Variablen

Bei der linearen Regression werden die Variablenausprägungen $x_i$ mit den Koeffizienten $\hat{\beta}_i$ multiplziert und addiert. Es muss sich daher bei den Ausprägungen um Zahlenwerte handeln - kategoriale Variablen verwenden aber oft alphanumerische Bezeichnungen für die Ausprägungen. Und selbst wenn die Ausprägungen über Zahlen codiert werden, lässt sich mit diesen Zahlen nicht sinnvoll rechnen. Wir benötigen also einen Weg, die Ausprägungen kategorialer Variablen in Rechengrößen umzuwandeln. Dies wird durch verschiedene Formen der *Codierung* geleistet. Es gibt verschiedene Formen der Codierung, die sich hinsichtlich ihrer statistischen Eigenschafen unterscheiden. Gebräuchlich, wenn auch nicht immer die beste Wahl, sind das *one-hot-encoding* und das *dummy-encoding*. Beim one-hot-encoding einer kategorialen Variablen $x_i$ führt man für jede Ausprägung $\ell$ eine neue Variable $x_i^\ell$ ein, die über die Werte 0 und 1 für jede Beobachtung codiert, ob die ursprüngliche Ausprägung $\ell$ war (1) oder nicht (0). Die neuen Variablen sind also Indikatorvariablen für die Ausprägungen. Bei $L$ verschiedenen Ausprägungen erhält man  damit beim one-hot-encoding $L$ neue Variablen, die die ursprüngliche Variable im Modell ersetzen. Allerdings ist diese Codierung redundant, denn es würden $L-1$ binäre Variablen ausreichen, um $L$ verschiedene Ausprägungen zu modellieren: Der Fall, dass alle $L-1$ Variablen den Wert 0 haben wird nämlich beim one-hot-encoding nicht genutzt, weil immer genau eine Variable den Wert 1 hat. Beim dummy-encoding hingegen wählt man eine Referenzausprägung und weist dieser implizit die Codierung durch $L-1$ 0-Werte zu. Dies hat auch den Vorteil, dass die durch one-hot-encoding eingeführte lineare Abhängigkeit zwischen den $L$ Variablen und der (impliziten) Variable für den $\hat{\beta}_0$-Term (man kann sich eine Variable $x_0=1$ denken) vermieden wird. Bei der dummy-Codierung muss man entscheiden, welche Ausprägung zur Referenzausprägung wird. Im Regressionsmodell entspricht $\hat{\beta}_0$ dann dem Mittelwert der Referenzausprägung. Es ist daher wichtig eine Ausprägung zur Referenzausprägung zu wählen, die im Datensatz häufig vorkommt - ansonsten ist der Mittelwert nicht valide.    

```{r recipe_step_dummy_encoding, eval = FALSE}
housing_rec %>% step_dummy(all_nominal(), one_hot = FALSE) -> housing_rec
```

### Konstante Variablen entfernen

Der nächste Schritt entfernt die Variablen, die entweder eine sehr kleine Varianz oder eine sehr stark dominierende Ausprägung haben, d.h. wir wenden die Funktion `step_nvz` sowohl auf numerische als auch auf kategoriale oder nominale Variablen an.  

```{r recipe_step_low_variance, eval = FALSE}
housing_rec %>% 
  step_nzv(all_predictors()) -> housing_rec
```

### Zentrieren und Skalieren numerischer Variablen

Bei der linearen Regression ist der Erwartungswert der Zielvariable $y$ gleich dem konstanten Term $\hat{\beta}_0$, wenn die Prädiktoren $x_i = 0$ sind. 0 ist aber für viele Anwendungen in der Realität ein außergewöhnlicher Wert, bspw. eine Größe von 0$m$, eine Fläche von 0 $m^2$, eine Zahl von 0 Zimmern usw. Es wäre viel intuitiver, wenn $y = \hat{\beta}_0$ für den Fall, dass die Prädiktoren ihren jeweiligen Durchschnittswert annehmen. Und genau das kann man erreichen, wenn man die Variablen zentriert, d.h. vom jeweiligen Wert den Durchschnitt abzieht, denn dann haben die (zentrierten) Variablen den Durchschnittswert 0.

Manchmal *skaliert* man die Variablen auch noch zusätzlich zur Zentrierung: Das bedeutet, dass man die Werte auch noch durch die Standardabweichung dividiert. In der Statistik nennt man beide Schritte zusammen *z-Transformation*. Damit sind die Verteilungen der Variablenwerte vergleichbar und die Koeffizienten $\hat{\beta}_i$ sind damit auch einfacher vergleichbar. Tatsächlich führt man durch den Schritt eine Einheitenumrechnung durch und alle Koeffizienten $\hat{\beta}_i$ haben dadurch dieselbe Einheit.

```{r recipe_scale_vars, eval = FALSE}
housing_rec %>% step_center(all_predictors()) %>%
  step_scale(all_predictors()) -> housing_rec
```


### Bereinigung um stark korrelierte Variablen

Wie oben erläutert, können stark korrelierte Variablen die Robustheit des Modells und dessen Interpretation verschlechtern. Daher entfernen wir stark korrelierte numerische Variablen:
```{r recipe_handle_correlations, eval = FALSE}
housing_rec %>% step_corr(all_predictors(), threshold = 0.8) -> housing_rec
```

## Modellbildung

```{r prep_model, include = FALSE}
housing_rec %>% 
  step_knnimpute(all_predictors(), neighbors = 3) %>% 
  step_other(all_nominal(), threshold = 0.05) %>% 
  step_dummy(all_nominal(), one_hot = FALSE) %>% 
  step_nzv(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = 0.8) -> housing_rec
```

### Trainingsdaten vorverarbeiten

Zuerst müssen wir die Trainingsdaten anhand des oben entwickelten *Rezepts* vorverarbeiten.
```{r prep_training, eval = FALSE}
prepared_recipe <- housing_rec %>% prep(retain = TRUE)

train_prepped <- prepared_recipe %>% juice()
```

### Modell erstellen
Ähnlich wie die Vorverarbeitung, erstellt man mit dem `parsnip`-Paket (wie `recipes` gehört dieses Paket der Paketfamilie `tidymodels` an) eine Spezifikation für ein Modell, das man dann mittels einer *engine*, d.h. einer konkreten Verfahrensimplementierung, berechnet.

```{r parsnip_model, eval = FALSE}
lr_mod <- linear_reg(mode = "regression") %>% set_engine("lm")
```

Das *Lernen* des Modells erfolgt dann über die `fit`-Funktion:
```{r fit_lr_model, eval = FALSE}
housing_fit <- fit(object = lr_mod, formula = formula(prepared_recipe), data = train_prepped)

the_model <- housing_fit$fit
summary(the_model)

```

### Modell evaluieren

```{r prep_model_eval, include = FALSE}
if (write_mode) {
  prepared_recipe <- housing_rec %>% prep(retain = TRUE)
  saveRDS(prepared_recipe, file = paste0(dir_pattern, "prepared_recipe.rds"))
} else {
  prepared_recipe <- readRDS(file = paste0(dir_pattern, "prepared_recipe.rds"))
}
train_prepped <- prepared_recipe %>% juice()
lr_mod <- linear_reg(mode = "regression") %>% set_engine("lm")
if (write_mode) {
  housing_fit <- fit(object = lr_mod, formula = formula(prepared_recipe), data = train_prepped)
  saveRDS(housing_fit, file = paste0(dir_pattern, "housing_fit.rds"))
} else {
  housing_fit <- readRDS(file = paste0(dir_pattern, "housing_fit.rds"))
}
the_model <- housing_fit$fit
```


Wir können verschiedene Metriken verwenden, um besser die Qualität des Modells (zunächst auf Basis der Trainingsdaten) zu untersuchen. Das Paket `yardstick`, das ebenfalls zu `tidymodels` gehört, bietet hier schon eine gute Auswahl:

```{r model_metrics, eval = FALSE}
pred_truth_train <- tibble(SalePrice = train_prepped$SalePrice, predicted = the_model$fitted.values)

metrics(pred_truth_train, SalePrice, predicted)

```

*rsq* steht hier für das Bestimmtheitsmaß $R^2$, *rmse* für *root mean squared error*, d.h. 
$\sqrt{\frac{1}{N}\sum_{j=1}^{N}(y^{(j)}- \hat{y}^{(j)})^2}$, und *mae* für den mittleren absoluten Fehler, d.h. $\frac{1}{N}\sum_{j=1}^{N}|y^{(j)}- \hat{y}^{(j)}|$. 

Bei linearer Regression empfiehlt es sich, die Ergebnisse graphisch zu veranschaulichen.

```{r viz_predictions, eval = FALSE}
pred_truth_train %>% ggplot(aes(x = SalePrice, y = predicted)) + geom_point() + geom_smooth(method = "lm")
```

Die Vorhersagefehler sollten, entsprechend der Modellannahme, normalverteilt sein:

```{r viz_residual_density, eval = FALSE}
residuals <- pred_truth_train %>% mutate(err = SalePrice - predicted)
residuals %>% ggplot(aes(x = err)) + geom_density()
```

Man verwendet häufig den *Q-Q-Plot*, um diese Annahme zu prüfen: Dabei werden die geordneten *standardisierten Fehler* über den erwarteten geordneten Werten der Standardnormalverteilung abgetragen. Zuerst müssen wir daher die standardisierten Fehler (s. @Sheather2009) berechnen:

```{r qqplot_on_std_res, eval = FALSE}
std_res <- stdres(the_model)

qqnorm(std_res)
abline(0, 1, col = "red")
```

Wir sehen, dass die standardisierten Residuen (Fehler) für einen Großteil des Wertebereichs schon sehr gut den Erwartungen, nämlich einer Standardnormalverteilung zu folgen, entsprechen. Aber es gibt auch Ausnahmen und wir prüfen daher die Standardresiduen abgetragen über den Indices der Beobachtungen. Zusätzlich plotten wir in rot den (skalierten) Verkaufspreis.

```{r viz_srdres_over_index, eval = FALSE}
idx_stdres <- tibble::enframe(std_res, name = "Index", value = "StdRes") %>% 
  mutate(Index = as.integer(Index)) %>% 
  bind_cols(train_prepped %>% select(SalePrice))


idx_stdres %>% 
  arrange(Index) %>% 
  ggplot(aes(x = Index, y = StdRes)) +
  geom_point() + 
  geom_linerange(aes(ymax = StdRes, ymin = 0)) +
  geom_label(aes(x = Index, y = StdRes, label = Index), size = 3, 
            data = idx_stdres %>% filter(abs(StdRes) > 5),
            nudge_y = 0.5) +
  geom_line(aes(x = Index, y = scale(SalePrice)), color = "red", alpha = 0.5) +
  scale_x_continuous(breaks = c(1, seq(200, 2200, 200)))
```

Das Modell scheint besonders bei höheren Verkaufspreisen auch größere Fehler aufzuweisen, was wir überprüfen:

```{r viz_stdres_over_target, eval = FALSE}
idx_stdres %>% 
  ggplot(aes(x = SalePrice, y = StdRes )) +
  geom_point(alpha = 0.3) 
```

An dieser Stelle kann man den Prozess der Modellbildung noch fortsetzen: 
* Weisen die hochpreisigen Immobilien noch Besonderheiten auf?
* Wie verhält es sich mit den Ausreissern im niedrigeren Preisbereich: Was zeichnet diese aus?
* Könnten wir noch weitere Merkmale erzeugen, bspw. indem wir die Grundfläche der Immobilie als zusätzliche Variable quadratisch eingehen lassen? 

Für den Moment belassen wir es aber dabei. Zukünftige Versionen dieses Tutorials werden diese Fragen angehen.

<div class="check">
<div class="center">
**Verständnischeck**
</div>
Wenn wir wissen, dass unser Modell für höhere Verkaufspreise mit höherer Wahrscheinlichkeit zu stark abweichende Vorhersagen liefert: 

* Welchen Einfluss hat das auf unseren Geschäftsprozess? 
* Wie könnten wir damit umgehen?
</div>

### Modell auf Testdaten anwenden

```{r prep_test_mode, include = FALSE}
if (write_mode) {
  test_prepped <- prepared_recipe %>% bake(new_data = test_data)
  saveRDS(test_prepped, file = paste0(dir_pattern, "test_prepped.rds"))
} else {
  test_prepped <- readRDS(file = paste0(dir_pattern, "test_prepped.rds"))
}

housing_pred <- predict(the_model, test_prepped)

pred_truth_test <- tibble::enframe(housing_pred, name = NULL, value = "predicted") %>%
  bind_cols(test_data %>% select(SalePrice)) %>% 
  mutate(err = SalePrice - predicted)

plot_var_imp <- function(mdl) {
  mdl %>% tidy() %>% filter(term != "(Intercept)") %>% mutate(sign = as.factor(sign(statistic)), imp = abs(statistic)) %>% 
  top_n(n = 15, wt = imp) %>% 
  ggplot(aes(x = reorder(term, imp), y = statistic, fill = sign)) + geom_col() + coord_flip() +
  scale_fill_discrete() + guides(fill = FALSE) +
  labs(x = "Variable", y = "t-Statistik")
}
```

Der wichtigste Test für das Modell ist aber, ob es Verallgemeinern kann, d.h. ob es auf zuvor nicht gesehenen Daten gute Vorhersagen liefert. Dem trainierten Modell werden dazu die zuvor dem Modell unbekannten Testdaten übergeben und das Modell berechnet einen Schätzwert für die Zielvariable. Allerdings kann man dem Modell die Testdaten nicht einfach in der Form übergeben, in der die Daten vorlagen: Stattdessen muss man die Vorverarbeitungsschritte, die man für den Trainingsdatensatz durchgeführt hat, auch für die Testdaten anwenden. Insbesondere müssen auch im Testdatensatz die Variablen zentriert und skaliert werden, weil dies für den Trainingsdatensatz auch durchgeführt wurde: Für die Vorverarbeitung des Testdatensatzes werden also die Mittelwerte und Standardabweichungen der entsprechenden Variablen des Trainingsdatensatzes benötigt. Ohne die Verwendung der Methoden aus dem `tidymodels`-Paket, müssten wir uns diese Parameter an geeigneter Stelle merken und anwenden. Dieser Schritt wird uns von den `tidymodels`-Methoden abgenommen, so dass wir für das Testen des Modells einfach schreiben können:

```{r predict_housing, eval = FALSE}
test_prepped <- prepared_recipe %>% bake(new_data = test_data)

housing_pred <- predict(the_model, test_prepped)
```

Die Vorhersagegüte auf Testdaten 
```{r preview_predictions, eval = FALSE}
pred_truth_test <- tibble::enframe(housing_pred, name = NULL, value = "predicted") %>%
  bind_cols(test_data %>% select(SalePrice)) %>% 
  mutate(err = SalePrice - predicted)
```

bestimmen wir einmal anhand von Metriken:
```{r preview_predictions_metrics}
metrics(pred_truth_test, truth = SalePrice, estimate = predicted)
```

und auch graphisch:

```{r preview_predictions_chart}
pred_truth_test %>% ggplot(aes(x = SalePrice, y = predicted)) + geom_point() + geom_smooth(method = "lm")
```

Die Verteilung der Fehler der Testdaten können wir uns im Vergleich zu den Trainingsdaten ansehen:
```{r density_test_errors_vs_training}

ggplot(mapping = aes(x = err)) + geom_density(data = residuals, color = "green") + geom_density(data = pred_truth_test, color = "red") 

```

Aufgrund der Metriken und Graphiken kommen wir zu dem Schluss, dass das Modell gut generalisiert -- die oben genannten Verbesserungsmöglichkeiten bestehen natürlich nach wie vor.

## Modellinterpretation

Wenn wir dem Management die Aussage präsentieren, dass unser Modell einen $R^2$-Wert von 0.86 erreicht, werden wir mit hoher Sicherheit nur fragende Blicke ernten: Stattdessen wird man wissen wollen, was denn nun den Preis einer Immobilie bestimmt. Und genau diese Frage nach der Erklärbarkeit des Modells wird uns als Data Scientists immer wieder begegnen.

Eine Möglichkeit, diese Frage zu beantworten, besteht darin, die Wichtigkeit von Variablen zu messen.
Für valide lineare Modelle gibt der Betrag des $t$-Werts, d.h. der Wert des geschätzten Koeffizienten $\hat{\beta}_i$ geteilt durch den zugehörigen Standardfehler, ein Maß für die Wichtigkeit der entsprechenden Variablen an. Der Wert (allerdings mit Vorzeichen) wird im `summary` des linearen Modells angegeben und wir können mit ein paar einfachen Befehlen eine Liste der wichtigsten Variablen erhalten: 

```{r table_var_imp}
summary(the_model) %>% tidy() %>% mutate(varImp = abs(statistic)) %>% arrange(desc(varImp))

```

Die Wichtigkeit der Variablen wird über den Betrag der $t$-Statistik gemessen. Weiterhin interessant ist aber auch die Richtung, in die eine Variable wirkt: Diese wird über das Vorzeichen des Koeffizienten (und damit auch der $t$-Statistik) bestimmt. Graphisch können wir uns die Wichtigkeit und Richtung der Variablen über ein paar Zeilen `dplyr` und `ggplot2`-Code erzeugen:

```{r var_imp_plot_func, eval = FALSE}
plot_var_imp <- function(mdl) {
  mdl %>% tidy() %>% filter(term != "(Intercept)") %>% mutate(sign = as.factor(sign(statistic)), imp = abs(statistic)) %>% 
  top_n(n = 15, wt = imp) %>% 
  ggplot(aes(x = reorder(term, imp), y = statistic, fill = sign)) + geom_col() + coord_flip() +
  scale_fill_discrete() + guides(fill = FALSE) +
  labs(x = "Variable", y = "t-Statistik")
}
```

und dann anwenden:
```{r var_imp_plot}
plot_var_imp(the_model)
```

Statistisch kann jedem Koeffizienten $\hat{\beta}_i$ auch ein Konfidenzintervall zugeordnet werden. Das Paket `sjPlot` bietet die Funktion `plot_model`, das für alle (oder eine Auswahl) von Variablen die Schätzwerte $\hat{\beta}_i$ und deren Konfidenzintervalle darstellt:

```{r chart_model_imp}
plot_model(the_model, type = "est", sort.est = TRUE, vline.color = "black")

```

Bei den Variablen, deren Konfidenzintervall die 0 einschließt, können wir nicht sicher sein, dass sie immer zu einer guten Schätzung der Zielvariablen, also des Verkaufspreises, beitragen. Diese Variablen sollten wir daher nicht für ein produktives Modell wählen.

*Anmerkung* Wir sollten eigentlich bei `type` den Wert `std` wählen, damit wir die standardisierten Koeffizienten angezeigt bekommen. Aus ungeklärter Ursache, funktioniert das hier aber nicht.

## Alternative Modelle
Anstelle Linearer Regression könnten wir verschiedene andere Modelltypen verwenden: *Entscheidungsbäume*, *Random Forests*, *Gradient Boosting*, *neuronale Netze* usw. Aber angesichts eines schon erreichten Bestimmtheitsmaßes von 0.86 können wir nicht allzu große Verbesserungen erwarten. Und von so einer kleinen Verbesserung wird unsere Entscheidung für oder gegen die Übernahme des Maklerauftrags nicht abhängen. Wir können Verbesserungen eher auf andere Weise erreichen:
* Können wir ein hinreichend gutes Modell erstellen, dass mit weniger Variablen auskommt? Je weniger Variablen, desto weniger Eingaben benötigt unser Tool und der Prozess läuft schneller und sicherer.
* Können wir uns vielleicht auf rein numerische Variablen beschränken? Das wäre auch leichter zu benutzen, weil Kategorien nicht immer eindeutig sind.

### Rein numerische Variablen

```{r prep_model_only_num, include = FALSE}
recipe(SalePrice ~ ., data = training_data) %>% 
  step_rm(all_nominal()) %>% 
  step_knnimpute(all_predictors(), neighbors = 3) %>%  
  step_nzv(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = 0.8) -> housing_rec_num

if (write_mode) {
  prepared_recipe_num <- housing_rec_num %>% prep(retain = TRUE)
  saveRDS(prepared_recipe, file = paste0(dir_pattern, "prepared_recipe_num.rds"))
} else {
  prepared_recipe_num <- readRDS(file = paste0(dir_pattern, "prepared_recipe_num.rds"))
}

train_prepped_num <- prepared_recipe_num %>% juice()

lr_mod_num <- linear_reg(mode = "regression") %>% set_engine("lm")

if (write_mode) {
  housing_fit_num <- fit(object = lr_mod_num, formula = formula(prepared_recipe_num), data = train_prepped_num)
  saveRDS(housing_fit_num, file = paste0(dir_pattern, "housing_fit_num.rds"))
} else {
  housing_fit_num <- readRDS(file = paste0(dir_pattern, "housing_fit_num.rds"))
}

the_model_num <- housing_fit_num$fit

```


Zunächst probieren wir aus, wie gut ein lineares Regressionsmodell wird, wenn wir uns auf numerische Daten beschränken.
Wir gehen dazu analog wie oben vor:

```{r create_numerical_recipe, eval = FALSE}
recipe(SalePrice ~ ., data = training_data) %>% 
  step_rm(all_nominal()) %>% 
  step_knnimpute(all_predictors(), neighbors = 3) %>%  
  step_nzv(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = 0.8) -> housing_rec_num
```

Das Training des Modells ist ebenfalls analog zu oben:

```{r train_numerical_recipe, eval = FALSE}
prepared_recipe_num <- housing_rec_num %>% prep(retain = TRUE)

train_prepped_num <- prepared_recipe_num %>% juice()

lr_mod_num <- linear_reg(mode = "regression") %>% set_engine("lm")

housing_fit_num <- fit(object = lr_mod_num, formula = formula(prepared_recipe_num), data = train_prepped_num)

the_model_num <- housing_fit_num$fit
```

Die Ergebnisse können wir uns wieder als `summary`
```{r train_numerical_recipe_summary}
summary(the_model_num)
```

und als Plot unter Wiederverwendung der obigen Funktion ansehen:
```{r train_numerical_recipe_plot}
plot_var_imp(the_model_num)

```

Die Modellqualität (gemessen anhand des Bestimmtheitsmaßes) ist also doch deutlich niedriger als beim ersten Modell mit kategorialen Variablen.

### Modell mit Regularisierung

```{r prep_lasso_model, include = FALSE}
lasso_mod <- linear_reg(mode = "regression", mixture = 1) %>% set_engine("glmnet") 

if (write_mode) {
  housing_fit_lasso <- fit(object = lasso_mod, formula = formula(prepared_recipe), data = train_prepped)
  saveRDS(housing_fit_lasso, file = paste0(dir_pattern, "/housing_fit_lasso.rds"))
} else {
  housing_fit_lasso <- readRDS(file = paste0(dir_pattern, "housing_fit_lasso.rds"))
}

the_model_lasso <- housing_fit_lasso$fit

lasso_tab <- tibble(lambda = the_model_lasso$lambda, nonzerovars = the_model_lasso$df)

pred_lasso <- predict.model_fit(housing_fit_lasso, new_data = train_prepped)

pred_lambda_rmse <- pred_lasso %>% 
  mutate(truth = rep(train_prepped$SalePrice, length(the_model_lasso$lambda)),
         err = truth - .pred_values,
         lambda = .pred_lambda) %>% 
  group_by(lambda) %>% summarise(RMSE = sqrt(mean(err**2)))

scalef <- max(pred_lambda_rmse$RMSE) / max(lasso_tab$nonzerovars)

```


Das Beispiel mit nur numerischen Variablen zeigt, dass man durchaus Gestaltungsmöglichkeiten hat, dabei aber evtl. eine Veränderung der Modellqualität in Kauf nehmen muss. Grundsätzlich ist es wünschenswert ein einfaches und gutes Modell zu haben. Ein einfaches Modell ist dabei (zumindest im Fall der linearen Regression) eines mit wenigen Variablen. Die *LASSO*-Methode (s. @James2013), wobei LASSO für *Least Absolute Shrinkage and Selection Operator* steht, ist eine scheinbar kleine Erweiterung der linearen Regression, mit der die zwei Teilprobleme Variablenselektion und Modelloptimierung simultan gelöst werden können: *Regularisierung*.

Die Zielfunktion von LASSO lautet 
$$ \min_{\beta_0, \beta_1,\ldots,\beta_n} \left(\sum_{j=1}^N \left(y^{(j)} - \left(\beta_0 + \sum_{i=1}^n \beta_i\cdot x_i^{(j)}\right)\right)^2 + \lambda\cdot\sum_{i=1}^n|\beta_i|\right). $$
Im Vergleich zur Zielfunktion der normalen linearen Regression gibt es also einen zusätzlichen Parameter $\lambda\in\mathrm{R}$ und eine Summe der Absolutwerte der Koeffizienten $\beta_i$. Wichtig ist: $\lambda$ ist ein wählbarer Parameter und keine Variable, die im Lösungsprozess bestimmt wird. Mit dem Parameter $\lambda$ kann man steuern, wie stark die Minimierung der Zielfunktion die Reduktion der Modellkomplexität durch Nullsetzen von einem oder mehreren $\beta_i$ im Vergleich zur Minimierung des quadratischen Fehlerterms anstrebt. Zu den mathematischen Hintergründen empfiehlt sich das auch ansonsten empfehlenswerte Buch @James2013 (kostenlos erhältlich unter http://faculty.marshall.usc.edu/gareth-james/ISL/).

Für die Anwendung von LASSO in unserem Fall, müssen wir bei der Wahl der Methode ein paar Parameter anpassen:
```{r setup_lasso_model}

lasso_mod <- linear_reg(mode = "regression", mixture = 1) %>% set_engine("glmnet") %>% translate()

housing_fit_lasso <- fit(object = lasso_mod, formula = formula(prepared_recipe), data = train_prepped)

the_model_lasso <- housing_fit_lasso$fit

```

Die zugrunde liegende Implementierung aus dem `glmnet`-Paket bestimmt eine Folge von $\lambda$-Werten und ermittelt für diese die Koeffizienten und weitere Kennwerte. Aus dem Ergebnis können wir entnehmen, wie sich die Zahl der Koeffizienten $\beta_i$ ungleich 0 in Abhängigkeit vom Wert für $\lambda$ ergibt:

```{r lasso_nonzeros}
lasso_tab <- tibble(lambda = the_model_lasso$lambda, nonzerovars = the_model_lasso$df)

lasso_tab %>% ggplot(aes(x = log(lambda), y = nonzerovars)) + geom_line(color = "red")

```

Diese Entwicklung der Zahl genutzter Variablen können wir nun der Qualität des Modells gegenüberstellen. Zuerst wenden wir das Modell auf die Trainingsdaten selbst an, um daraus dann den Trainingsfehler zu bestimmen und ihn anzuzeigen.

```{r chart_lasso_rmse_lambda}
pred_lasso <- predict.model_fit(housing_fit_lasso, new_data = train_prepped)

pred_lambda_rmse <- pred_lasso %>% 
  mutate(truth = rep(train_prepped$SalePrice, length(the_model_lasso$lambda)),
         err = truth - .pred_values,
         lambda = .pred_lambda) %>% 
  group_by(lambda) %>% summarise(RMSE = sqrt(mean(err**2)))

scalef <- max(pred_lambda_rmse$RMSE) / max(lasso_tab$nonzerovars)
ggplot() + geom_line(data = pred_lambda_rmse, aes(x = log(lambda), y = RMSE), color = "blue") +
  geom_line(data = lasso_tab, aes(x = log(lambda), y = scalef * nonzerovars), color = "red") +
  scale_y_continuous("RMSE", sec.axis = sec_axis(~ ./scalef, name = "# Variablen"))

```

Wenn wir das Modell für einen speziellen $\lambda$-Wert haben wollen, müssen wir die entsprechenden Strukturen aus dem Modellobjekt entnehmen:
```{r lasso_non_zeros}
my_lambda = exp(9)
one_mod <- coef(housing_fit_lasso$fit, s = my_lambda)

non_zero_coeffs <- one_mod[which(one_mod[, 1] != 0),] 

(non_zero_coeffs %>% enframe(name = "Variable", value = "Koeffizient"))

```

Aus der Graphik oben sehen wir, dass unser Modell mit `r nrow(non_zero_coeffs) - 1` Variablen einen RMSE von ca. 40.000 verglichen mit ca. 30.500 des linearen Modells auf den Trainingsdaten hat. Dafür verwendete das Modell `r length(which(the_model$coefficients != 0))` Variablen. 


## Schlussbemerkung

Damit endet dieses Tutorial. Wir haben einige wichtige Themen der Modellentwicklung und -evaluierung behandelt. Die meisten dieser Themen sind für andere Methoden in ähnlicher Weise durchzuführen. Erfahrungsgemäß wird die Qualität von Modellen durch eine sorgfältige Vorarbeit und Modellvalidierung sehr viel besser als durch den (alleinigen) Wechselzu einem (vermeintlich) mächtigeren Modelltyp.

Durch Ihre Rückmeldung wird dieses Tutorial in Zukunft Schritt für Schritt besser. Rückmeldung daher bitte an [andreas@cardeneo.de](mailto://andreas@cardeneo.de).




## Session Info
```{r sessioninfo}
sessionInfo()
```




