--- 
title: "Vorlesung Data Science Praxis"
author: "Andreas Cardeneo"
date: "`r Sys.Date()`"
site: "bookdown::bookdown_site"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
documentclass: book
bibliography: ["citation.bib"]
biblio-style: "apalike"
link-citations: true
---

# Vorlesung Data Science

*Deutsch, damit es als Arbeitsblatt geeignet ist*

In diesem Kapitel geht es um Techniken, um *valide* Modelle zu erstellen. Valide Modelle erfüllen die Voraussetzungen der Methoden, die für ihre Analyse verwendet werden. Wir werden hier zunächst mit Lineare Regression arbeiten, weil diese Methode gut verständlich und theoretisch fundiert ist und ihre Ergebnisse gut *erklärbar* sind. Das heißt nicht, dass diese Method auch immer *geeignet* ist, aber in vielen praktischen Fällen kann sie ein guter erster Ansatz sein.

Wir werden einen im Internet und für R verfügbaren Datensatz benutzen, den *Ames Iowa Housing" Datensatz (s. @R-AmesHousing). Dabei handelt es sich um Verkaufsdaten zu Wohnungen und Gebäuden sowie um beschreibende Daten der Immobilien. Es handelt sich damit um ein durchaus anwendungsnahes Szenario.

## Das Szenatio

Die *HaiBank AG* möchte im Niedrigzinsumfeld und angesichts steigender Immobilienpreise ihr eigenes Angebot als Immobilienmakler ausbauen. Unter der Marke *ImmoHai* sollen Maklerdienste für Immobilien angeboten werden und eine wichtige Komponente dabei ist die Abschätzung der erwarteten Profitabilität der Vermittlung einer Immobilie. Neben dem eigenen Aufwand auf der Kostenseite ist dafür der erzielbare Verkaufspreis maßgeblich. Traditionell wurde dieser Verkaufspreis von Immobilienexperten nach einer Begehung des Objekts und anhand der Erfahrung eingeschätzt. Weil die Unternehmensstrategie einen deutlichen Ausbau des Maklergeschäfts anstrebt, ist dieser Ansatz aufgrund des hohen Aufwands nicht mehr zu leisten. Stattdessen soll es, ähnlich wie bei der Bonitätsprüfung für Hauskredite, eine Software geben, die anhand von Kennzahlen der Immobilie einen realisierbaren Verkaufspreis prognostiziert. Diese Software soll von Sachbearbeitern im Kundengespräch verwendet werden und dabei helfen zu entscheiden, ob die *HaiBank* als Makler für die betreffende Immobilie auftreten soll.

### Aufgabenstellung

Ihre Aufgabe besteht darin, ein **Prognosemodell für Immobilienpreise** und einen Prototyp für die zugehörige **Anwendung** zu entwickeln, die dieses Prognosemodell verwendet.

Weil Ihre hauseigene IT auf absehbare Zeit noch damit beschäftigt ist, die bislang getrennt in jeder Filiale in Excel-Dateien festgehaltenen Immobiliengeschäfte in eine zentrale Datenbank zu überführen, haben Sie sich aus dem Internet eine frei verfügbare Datensammluung von Immobilienpreisen besorgt. Mit Hilfe dieser Daten können Sie schon einmal den ganzen Prozess der Modellentwicklung durchlaufen und einen Prototypen erstellen, damit Ihr Management auch nachvollziegen kann, wie die fertige Lösung aussehen wird.

Als technische Plattform stehen Ihnen 

* die Programmiersprache *R*, 
* die Entwicklungsumgebung *RStudio* und 
* das auf *R* basierende Framework *flexdashboard* 

für die Entwicklung von Modell und Anwendung zur Verfügung.

### Setup

Um mit *R* zu starten bietet es sich an, ein Konto bei *RStudio Cloud* anzulegen. Damit haben Sie eine *RStudio* Entwicklungsumgebung, die Sie über Ihren Browser nutzen können und eine Basisinstallation von *R* als Ausgangsbasis. Im Rahmen dieses Kurses können Sie einem bereits bestehenden *Workspace* beitreten (Anleitung s. unten). 

Alternativ können Sie *R* und *RStudio* auch lokal auf Ihrem Rechner installieren. Wie dies gemacht wird, erfahren Sie unter [R und RStudio installieren](https://rstudio.com/products/rstudio/download/).

#### RStudio Cloud Workspace
Hier die Erklärung, wie das funktioniert

### Grundsätzliches zur Arbeit mit *R*

Neben Python ist *R* eine der derzeit populärsten Sprachen für Datenanalye, Statistik und maschinellem Lernen. Während Python als vielseitige Programmiersprache gerne von einer Community eingesetzt wird, die aus der Softwareentwicklung kommt, hat *R* seinen Ursprung in der statistischen und wissenschaftlichen Community und bietet sehr vielfältige Möglichkeiten für (explorative) Datenanalyse und statistische Methoden. Zu den letzteren kann man im weiteren Sinne auch Methoden des maschinellen Lernens zählen.

## Lösungskonzeption

wie soll die Lösung am Ende aussehen? Wie kommt man dahin?
welche Daten wird man brauchen? Welche Visualisierungen?
Haben wir Hypothesen?

## Datenzugang

### Datenschutz DSGVO

Rechtsgrundlage und Verarbeitungszweck
Anonymisierung und Pseudonymisierung


## Explorative Datenanalyse

Wenn man eine neue Datenquelle erschließen möchte, bestehen die ersten Schritte aus der *explorativen Datenanalye*. Hierbei geht es darum, ein Verständnis für die Daten zu bekommen und insbesondere Fehler und Auffälligkeiten zu entdecken.

Wir laden zunächst die Immobiliendaten über ein *R*-Paket:
```{r}
library(AmesHousing)
```

Die Daten werden wir unter der Bezeichnung `raw` weiter untersuchen:
```{r}
raw <- ames_raw
names(raw) <- make.names(names(raw))
```

Die zweite Zeile im obigen Codeblock dient dazu, die Bezeichner der Variablen (Spalten des Datensatzes) so umzuwandeln, dass keine Leerzeichen mehr enthalten sind, die uns sonst an späterer Stelle stören würden.

Um uns einen ersten Eindruck von den Daten zu verschaffen, benutzen wir die Funktion `glimpse` aus dem `dplyr`-Paket. Dieses Paket ist grundsätzlich eine gute Wahl für die Datenanalyse, weil es zahlreiche recht intuitiv anwendbare Funktionen für Datentransformationen, Datenfilterung und -aggregation bietet.

```{r}
library(dplyr)

glimpse(raw)
```

Aus dieser Übersicht erfahren wir schon eine ganze Menge:

* die Zahl der Datensätze
* die Zahl der Merkmale
* die Namen der Merkmale 
* die Datentypen der Merkmale
* eine Vorschau auf die Merksmalsausprägungen.

    **Aufgabe**
    Was bedeuten die Ausprägungen *NA* für die Variable *Alley*. 


    **Aufgabe**
    Was bedeutet die Typkennzeichnung `fct` in der Ausgabe von `glimpse`? Was unterscheidet eine *kategorielle* Variable von numerischen? Was wäre der Unterschied, wenn die Variable *Heating* als Zeichenkette (`char` bzw. `character`) abgebildet würde?
    
### Erstes Verständnis der Daten

Mit `glimpse` konnten wir einen ersten Eindruck der Daten gewinnen. Die zahlreichen Einzelwerte bieten aber noch keinen guten Überblick. Dafür ist eine stärker aggregierte Darstellung besser geeignet. Das Paket `dlookr` hilft dabei:
```{r}
library(dlookr)
```

Angewandt auf unsere Immobiliendaten erhalten wir:
```{r}
diagnose(raw)
```

        **Aufgabe**
        * Was können Sie aus der *unique_rate* von 1 für *PID* ableiten? 
        * Was sagt Ihnen die *unique_rate* von 16 für die Variable *MS.SubClass*?
        
#### Zusammenfassung numerischer Daten

Wir können zu numerischen Werten mittels der Funktion `diagnose_numeric` mehr Informationen zu den Variablen erhalten, denen ein numerischer Datentyp zu Grunde liegt:

```{r}
diagnose_numeric(raw)
```

#### Zusammenfassung kategorieller Daten

Für kategorielle Variablen lassen sich keine sinnvollen Quantile und auch kein Mittelwert angeben. Hier sieht die Zusammenfassung für die Variable *MS.Zoning* folgendermaßen aus:
```{r}
diagnose_category(raw, MS.Zoning)
```

*R* bietet übrigens standardmäßig, d.h. ohne weitere Pakete zu laden, die Funktion `summary`, die eine kompakte Übersicht über einen Datensatz bietet:
```{r}
summary(raw)
```

Aber man sieht schon deutlich den Unterschied: Die Ausgabe über die Funktionen des Pakets `dlookr` ist übersichtlicher und auch leichter weiterzuverarbeiten.

### Fehlende Werte

In vielen Datensätzen, die aus unternehmensinternen IT-Systemen bezogen werden, finden sich Datensätze mit Merkmalen ohne Ausprägung. Es gibt keine pauschale Vorgehensweise, die bzgl. der Behandlung dieser fehlenden Werte immer richtig ist. Zunächst kann es sich lohnen die Spalten (Variablen) in den Datensätzen zu identifizieren, die viele fehlende Werte aufweisen.

Gehen wir nochmal zurück zur obigen Diagnose:
```{r}
diagnose(raw)
```

Die Spalte *missing_percent* gibt den Anteil fehlender Werte an und wir können die Spalten mit häufig fehlenden Werten über eine Sortierung erkennen:
```{r}
diagnose(raw) %>% arrange(desc(missing_percent))
```

Deutlich wird, dass die Variable *Pool.QC* wohl häufig keine Werte enthält. Eine Vermutung wäre, dass diese Variable etwas mit dem Vorhandensein eines Pools zu tun hat und dass fehlende Werte schlichtweg bedeuten, dass die Immobilie keinen Pool hat. Etwas anders sieht es bei der Variable *Bsmt.Cond* aus. Diese fehlt in knapp 3% aller Fälle -- ebenso wie andere Variablen mit ähnlichem Namen. Vielleicht ist es hier ähnlich wie bei der Variablen *Pool.QC* und nicht vorhandene Werte bedeuten einen fehlenden Keller. Aber kann es sein, dass bei ca. 3% aller Immobilien kein Keller vorhanden ist? Um dies besser zu verstehen, müsste man mehr über die Bauweise in der Region in Erfahrung bringen: Vielleicht wurden früher bei der Erfassung der Immobiliendaten keine Angaben zum Keller aufgenommen und die Daten fehlen schlichtweg für ältere Immobilien?

Unabhängig davon, was hier im konkreten Fall Ursache für die fehlenden Werte ist: Bei der explorativen Datenanalyse müssen wir stets berücksichtigen, dass das Fehlen selbst auch eine fachliche Information (hier: *kein Pool vorhanden*) bedeuten kann und nicht immer auf Fehler bei der Datenaufnahme oder -übertragung zurückzuführen ist bzw. mit Änderungen in der Erfassungssystematik zu tun haben muss.

Wir haben durch die obige Analyse schon den Verdacht, dass fehlende Werte für mehrere Variablen in typischen Kombinationen vorkommen: Wenn es keinen Keller gibt oder keine Werte erfasst wurden, dann fehlen wahrscheinlich Werte für alle diesbezüglichen Variablen. Um das zu untersuchen, können wir das Paket `VIM` verwenden:

```{r}
library(VIM)
```

Wenden wir es auf unsere Daten an, wobei wir uns auf die Variablen beschränken, die laut vorangehender Analyse fehlende Werte aufweisen:
```{r}
missing_cols <- diagnose(raw) %>% filter(missing_percent > 0.0) %>% pull(variables)

raw %>% select_at(vars(all_of(missing_cols))) %>% aggr(combined = TRUE, sortVars = TRUE, cex.axis = 0.5)
```

Die für diese Untersuchung gewählte Beschränkung auf Variablen, die fehlende Werte aufweisen, birgt allerdings ein Problem: Auf diese Weise entgehen uns Zusammenhänge zwischen fehlenden Werten und anderen Variablen, die möglicherweise auf einen systematischen Zusammenhang schließen lassen. Vielleicht fehlen ja die Angaben zu Kellern für Gebäude bestimmter Jahrgänge oder aus bestimmten Gebieten, deren geologischen Eigenschaften keinen Kellerbau zulassen?

Gehen wir einem konkreten Verdacht nach, dass fehlende Werte für die *Bsmt*-Variablen mit der Lage der Immobilie zu tun haben. Hierfür können wir die Variable *Neighborhood* und die Variable *Bsmt.Cond* verwenden:

```{r}
library(ggplot2)

raw %>% dplyr::select(Neighborhood, Bsmt.Cond) %>% 
    mutate(has_basement = !is.na(Bsmt.Cond)) %>% 
    count(Neighborhood, has_basement) %>% 
    ggplot(aes(x = Neighborhood, y = n, fill = has_basement)) +
    geom_col(position = "fill") + 
    coord_flip() +
    scale_y_continuous(labels = scales::percent)

```

Anscheinend ist der Verdacht eines geographischen Zusammenhangs nicht unbegründet und das hilft uns, die Daten besser zu verstehen.

#### Systematisierung der Ursachen fehlender Werte

In @Sauer2019 (und auch an anderen Stellen) werden die Ursachen für fehlende Werte systematisch beschrieben. Im wesentlichen werden drei verschiedene Fälle unterschieden:

* Missing completely at random (MCAR),
* Missing at random (MAR),
* Missing not at random (MNAR)

Der erste Fall (MCAR) liegt vor, wenn keine bekannte bzw. erkennbare Ursache für das Fehlen vorliegen, d.h. das Fehlen ist insbesondere nicht auf die Ausprägungen anderer Variablen für dieselbe Instanz (dieselbe Beobachtung) zurückzuführen. Typische Ursachen hierfür wären fehlende Daten aufgrund von technischen Übermittlungsfehlern, die nichts mit der Beobachtung selbst zu tun haben. 

Das andere Extrem in dieser Einteilung ist die Klasse MNAR: Hier hängt die Wahrscheinlichkeit für das Fehlen der Ausprägung einer Variablen für eine Beobachtung von der Ausprägung selbst ab. Die Variable *Lot.Frontage* (Bedeutung *Linear feet of street connected to property*) aus unserem Immobiliendatensatz würde hier passen: Die Ausprägung fehlt im Datensatz, sobald die wahre Ausprägung 0 lautet, weil das Grundstück nicht an eine Straße angrenzt.

Dazwischen ist die Klasse MAR. Die Ursache für fehlende Werte liegt hier nicht in der Ausprägung der Variablen selbst, sondern hängt mit anderen Variablen zusammen. Die Variable *Garage.Yr.Blt* (Bedeutung: Baujahr der Garage) fehlt bspw. dann, wenn es keine Garage gibt. Das Fehlen liegt aber nicht am Baujahr selbst.

Die Systematisierung ist kein Selbstzweck. Wenn wir wissen, dass der MNAR-Fall vorliegt, sollten wir über eine neue Variable, die wir selber einführen, unser Wissen über die Gründe für das Fehlen explizit in unseren Datensatz aufnehmen. Im Beispiel könnten wir eine kategorielle Variable *street_access* mit den Werten 0 für *keinen Straßenzugang* und 1 für *mit Straßenzugang* einführen. 
Im Fall von MCAR kann man die fehlenden Werte evtl. gezielt ergänzen. Dieses Vorgehen wird *Imputation* genannt und es gibt verschiedene Verfahren dafür. Andererseits kann man die entsprechenden Instanzen evtl. auch löschen, d.h. für die weitere Betrachtung ignorieren, weil ihr Fehlen keine statistischen Verzerrungen einführt. 
Bei fehlenden Werten nach MAR würde das Löschen von Instanzen allerdings zu statistischen Verzerrungen führen. Im Beispiel hier würde man alle Immobilien ohne Garage entfernen, wenn man die Instanzen löscht, für die die Variable *Garage.Yr.Blt* keine Ausprägung hat. Damit hätte man kein repräsentatives Bild des Immobilienmarkts mehr.

##### Fehlende Werte in der Praxis

In der Praxis, gerade bei der Verwendung von Daten aus operativen IT-Systemen, wird das Fehlen von Werten oftmals durch besondere Ausprägungen codiert. Ein Grund dafür ist, dass die Benutzeroberfläche der IT-Systeme Eingaben in den entsprechenden Feldern erwartet, die der oder die SachbearbeiterIn im Moment der Eingabe nicht leisten kann. In numerischen Feldern wird dann gerne ein Wert wie 99 oder 999 eingetragen, in Textfeldern wird ein Leerzeichen "\u237d", ein "-" oder "." eingetragen. 

#### Umgang mit fehlenden Werten

Wie auch in @Sauer2019 dargestellt wird, ist der richtige Umgang mit fehlenden Werten ein komplexes Thema. Simple Lösungen bestehen darin, die fehlenden Werte durch den Mittelwert der (numerischen) Ausprägungen der Variable zu ersetzen. Bei kategoriellen Variablen kann man analog dazu den Modalwert, d.h. die häufigste Ausprägung, als Ersatzwert verwenden. Aber beide Vorgehen sind stark vereinfachend und können zu unrealistischen Merkmalskombinationen für die betroffenen Beobachtungen führen. Technisch kann so eine Ersetzung in R so umgesetzt werden:
```{r}
library(tidyr)

completed <- raw %>% mutate_if(is.numeric, ~ replace_na(., mean(., na.rm = TRUE)))
```

Die Funktion `replace_na` aus dem `tidyr`-Paket ersetzt *NA*-Werte durch den im zweiten Argument angegebenen Wert.

Um andere Imputationsstrategien kennenzulernen, wechseln wir zeitweise zu einem anderen Datensatz:
```{r}
library(MASS)
glimpse(whiteside)
```

Der Datensatz passt zum Immobilienszenario, weil er den Zusammenhang zwischen Gasverbrauch, Außentemperatur und Gebäudeisolierung zeigt.

```{r}
ggplot(whiteside, aes(x = Temp, y = Gas)) + geom_point() + 
    geom_smooth(method = "lm") + 
    facet_wrap(~ Insul)
```

Für das Ausprobieren entfernen wir zufällig einen kleinen Teil der Messungen des Gasverbrauchs aus den Daten:

```{r}
set.seed(123)

ws_mod <- whiteside %>% mutate(Gas = if_else(runif(nrow(whiteside)) < 0.15, NA_real_, Gas))
```

Im folgenden probieren wir aus, was passiert, wenn wir die  Imputationsstrategie *Imputation durch Mittelwert* wählen. Dazu verwenden wir das Paket `mice` (@VanBuuren2011).

Die Imputation durch den Mittelwert für alle numerischen Spalten kann durch folgenden Funktionsaufruf erreicht werden:

```{r} 
library(mice)

imp <- ws_mod %>% mice(method = "mean", m = 1, maxit = 1)
```

Das Ergebnis der Imputation können wir mit etwas Aufwand visualisieren (das hier geladene Paket `forcats` bietet Funktionen zur Bearbeitung von kategoriellen Variablen):
```{r}
library(forcats)

tmp1 <- imp$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    arrange(i) %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)

    
```

Aus fachlichen Gründen können wir davon ausgehen, dass es einen Zusammenhang zwischen Temperatur und Gasverbrauch gibt: Die Trendlinien oben legen das auch nahe.

```{r}
imp2 <- ws_mod %>% mice(method = "norm.predict", seed = 1, m = 1, print = FALSE)
```

Das Ergebnis dieser Imputation können wir auch wieder mit dem obigen Code visualisieren:
```{r}
tmp1 <- imp2$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)
```

Dieses Ergebnis sieht wesentlich plausibler aus als der erste Versuch auf Basis des Mittelwerts. 


### Ausreisser identifizieren

Wir kehren zurück zum *AmesHousing*-Datensatz.

Die obigen Diagnosetabellen bieten konkrete Information und man kann bspw. über den Vergleich des Durchschnitts mit dem Median einen ersten Eindruck der Verteilung der Werte bekommen. Weiterhin wichtig ist die Spalte *outlier*, die anzeigt, wie viele Ausreißer für die jeweilige Variable erkannt wurden. Die Definition, welche Werte als Ausreißer angesehen werden, gleicht der Definition von Ausreißern für *Boxplots* (siehe auch die Dokumentation zur Funktion `boxplot` mittels `?boxplot`).

Tatsächlich ist es sehr hilfreich, Daten und ihre Verteilungen zum besseren Verständnis zu visualisieren. Die Ausreißer der Variable *SalePrice* können wir folgendermaßen untersuchen:

```{r}
plot_outlier(raw, SalePrice)
```

Wie oben erwähnt, können kategorielle Variablen nicht gut numerisch zusammengefasst werden. Die Verteilung der Merkmalsausprägungen ist hingegen sehr wertvoll, weil schnell die häufigen und seltenen Ausprägungen erfasst werden können. Ein einfaches *Histogramm* lässt sich mit dem Paket `ggplot2` folgendermaßen erstellen:
```{r}
raw %>% ggplot(aes(x = MS.Zoning)) + geom_histogram(stat = "count")
```

### Seltene Werte

Aus Histogrammen können wir ableiten, welche Werte je Variable sehr selten auftreten. Gerade bei der Analyse kategorieller Variablen stellt sich oft heraus, dass einige Ausprägungen sehr selten auftreten. Eine statistische Analyse auf Basis dieser Ausprägungen würde statistisch wertlos sein, da die Anzahl der Beobachtungen pro Ausprägung nicht für statistisch valide Aussagen ausreicht. Daher ist es empfehlenswert und üblich, seltene Ausprägungen zu Gruppen zusammenzufassen und eine neue Dummy-Ausprägung, bspw. *sonstige*, für diese einzuführen.

Um eine vollständige Liste der kategoriellen Variablen zu erhalten, hilft die [Dokumentation des AmesHousing-Datensatzes](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). Übrigens finden sich auf der Seite [Gimme Shelter](https://www.gimme-shelter.com/category/term-2/) recht gute Erklärungen einiger (amerikanischen) Fachbegriffe der Immobilienbranche. 

In der Datendokumentation sehen wir, dass kategorielle Variablen als *Nominal* oder *Ordinal* aufgeführt werden. Wir könnten jetzt manuell eine Liste anlegen, aber als Data Scientists machen wir das natürlich nicht, sondern extrahieren die notwendige Information als Datensatz (die Basis für den hier verwendeten Code kommt von [Ron Sarafian](http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html)).

```{r}
library(stringr)
library(purrr)

doku <- url("http://jse.amstat.org/v19n3/decock/DataDocumentation.txt", encoding = "latin1") %>% readLines() 
data_type_lines <- grep("Nominal|Ordinal|Discrete|Continuous", doku, value = TRUE)

matches <- data_type_lines %>% str_match_all("^(.+)( |\\s)\\((.+)\\)(\\s*):") %>% compact %>% 
    map(~ c(VAR = .[,2], TYPE = .[,4])) %>% map_dfr(as.list) %>% 
    mutate(VAR = make.names(VAR))
```

Jetzt liegen die Spaltenbezeichnungen zusammen mit der Datenart als `data.frame` für die weitere Verarbeitung vor. Leider stellt sich heraus, dass die Dokumentation in einigen Fällen von den Spaltenbezeichnung im Datensatz abweicht, so dass wir diese Bezeichnungen manuell korrigieren müssen. Ein schönes Beispiel dafür, dass man bei der explorativen Datenanalyse immer wieder unerwartet auf Probleme stößt.

```{r}

categorials <- matches %>% filter(TYPE %in% c("Nominal", "Ordinal")) %>% dplyr::select(VAR) %>% 
    mutate(VAR = case_when(
        VAR == "Exterior.1" ~ "Exterior.1st",
        VAR == "Exterior.2" ~ "Exterior.2nd",
        VAR == "HeatingQC" ~ "Heating.QC",
        VAR == "KitchenQual" ~ "Kitchen.Qual",
        VAR == "FireplaceQu" ~ "Fireplace.Qu",
        VAR == "BsmtFinType.2" ~ "BsmtFin.Type.2",
        TRUE ~ VAR
    )) %>% pull() 

```

Zunächst modifizieren wir unseren Datensatz so, dass R die kategoriellen Variablen erkennt, indem wir die entsprechenden Spalten zu `factors` konvertieren.
```{r}
raw <- raw %>% mutate_at(vars(all_of(categorials)), as.factor) 
```


Jetzt können wir uns ansehen, wieviele verschiedene Ausprägungen wir für die kategoriellen Variablen haben. Nachdem wir schon wissen, dass *PID* eine eindeutige Kennung ist, lassen wir diese Variable aus der Betrachtung heraus. Für die graphische Darstellung bringen wir die Daten mit der `pivot_longer`-Funktion in ein *langes Format* (s. auch @Sauer2019), bei dem die Variablen nicht mehr einzeln in Spalten stehen. Zum besseren Verständis wird die Bildung des langen Datensatzes auf zwei Schritte mit zwischenzeitlicher Ausgabe der Tabelle aufgeteilt.

```{r} 
categ_freq <- raw %>% dplyr::select(all_of(categorials)) %>% dplyr::select(-PID) %>% 
    summarise_all(~ length(unique(.))) 

head(categ_freq)

categ_freq <- categ_freq %>% pivot_longer(cols = everything(), names_to = "Variable", values_to = "Werte")

head(categ_freq)

categ_freq %>% 
    ggplot(aes(x = reorder(Variable, Werte), y = Werte)) + geom_col() + coord_flip()
```

Im nächsten Schritt gucken wir uns alle kategoriellen Variablen an, die 6 oder mehr verschiedene Ausprägungen haben. Für diese erstellen wir jeweils ein Histogramm der Ausprägungen, um zu sehen, ob es seltene Ausprägungen gibt.
```{r}
rel_categs <- categ_freq %>% filter(Werte >= 6) %>% pull(Variable)

categ_levels <- raw %>% dplyr::select(all_of(rel_categs)) %>% map(~ fct_count(.))

```

Die Informationen über die Häufigkeit visualisieren wir: Weil alle unsere Plots verschiedene Ausprägungen haben und unterschiedlich viele Ausprägungen, bietet sich der üblich Weg in `ggplot` über `facet_wrap` oder `facet_grid` nicht an. Hier ist das Paket `patchwork` hilfreich, durch das wir eine Liste von Plots automatisch in einer Graphik anordnen können.
```{r fig.width = 15}
library(patchwork)

plot_level_freq <- function(idx, data) {
    data[[idx]] %>% ggplot(aes(x = reorder(f, n), y = n)) + geom_col() + coord_flip() + xlab(names(data)[[idx]])
}

var_cl_plots <- lapply(seq_along(categ_levels), plot_level_freq, categ_levels)

wrap_plots(var_cl_plots)
```
Zwar sind nicht alle Plots gut lesbar, aber wir sehen, was wir wissen wollten: Es gibt für viele kategorielle Variablen Gruppen von Ausprägungen, die sehr selten vorkommen.
Weiter unten bei der Datenvorverarbeitung für die Modellierung, werden wir daher solche kategoriellen Variablen entsprechend bearbeiten und seltene Ausprägungen zusammenfassen.

## Informationsbeiträge von Variablen

Nachdem wir uns im vorherigen Abschnitt mit kategoriellen Variablen befasst haben, kehren wir hier wieder zurück zu den numerischen Variablen. Wir sollten uns in Erinnerung rufen, das wir letztlich ein Modell für den Verkaufspreis von Immobilien entwickeln wollen. Dahinter steckt die Annahme, dass verschiedene Eigenschaften bzw. Eigenschaftskombinationen von Immobilien Einfluss auf den Preis haben. 

### Beitrag einzelner Variablen

Wir können davon ausgehen, dass Eigenschaften, die für alle Immobilien gleich oder zumindest sehr ähnlich sind, auf den Preis keinen Einfluss haben werden. Wenn wir also Variablen identifizieren, die wenig informativ sind, können wir diese von der weiteren Betrachtung ausschließen.

Wenn wir nur einzelne Variablen betrachten, können wir als Statistik für den Informationsgehalt numerischer Variablen die Varianz der Ausprägungen (Werte) verwenden. Die Spalte *Order* können wir dabei ignorieren, weil sie einfach nur eine Ordnungszahl enthält.

Als ersten Schritt visualisieren wir die Verteilungen (genauer: Dichtefunktionen) der numerischen Variablen. Nachdem wir oben schon die kategoriellen Variablen identifiziert haben, können wir die numerischen einfach als Differenz zur vollständigen Variablenmenge ermitteln:

```{r fig.width = 15}
library(tidyselect)

numeric_cols <- setdiff(names(raw), categorials)
```

Für die graphische Darstellung müssen wir den Datensatz wieder in ein *langes Format* bringen, analog dem Vorgehen weiter oben. 
```{r fig.width = 15}
raw_rs <- raw %>% select_at(all_of(numeric_cols)) %>% dplyr::select(-Order) %>% 
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Wert") 

plot_density <- function(data) {
    data %>% filter(!is.na(Wert)) %>% 
        ggplot(aes(x = Wert)) + geom_density() + xlab(first(data$Variable))
}

var_data <- raw_rs %>% group_split(Variable)

plots <- lapply(var_data, plot_density)

wrap_plots(plots)
```

Durch die Dichtediagramme können wir schon ahnen, welche Variablen wahrscheinlich eine niedrige Varianz aufweisen. Allerdings kann die Intuition auch täuschen, so dass wir die Varianzen der Variablen auch noch rechnerisch bestimmen:
```{r}
(nvar_var <- raw %>% select_at(all_of(numeric_cols)) %>% dplyr::select(-Order) %>% 
    summarise_all(~ var(., na.rm = TRUE) ) %>%     
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Varianz") %>% 
        arrange(Varianz))
```

Für die Datenvorverarbeitung werden wir Variablen entfernen, die eine niedrige Varianz haben. Umgekehrt können wir in der Analyse auch die Variablen ignorieren, die nur verschiedene Werte aufweisen - hier dürfte es sich fast immer um Datensatzschlüssel handeln.

### Zusammenhang zwischen Variablen: Korrelation

Zwischen metrischen Variablen bestehen oftmals mehr oder weniger stark ausgeprägte lineare Zusammenhänge. Dieser Zusammenhang lässt sich als *Korrelationskoeffizient* (man unterscheidet mehrere verschiedene Korrelationskoeffizienten, s. auch @Sauer2019. Hier verwenden wir den nach *Pearson*). Der Begriff *metrische Variable* bedeutet, dass die Variable intervall- oder verhältnisskaliert ist. Das gilt für stetige numerische Variablen und oftmals auch für diskrete numerische Variablen - im Prinzip immer dann, wenn man die Variable messen oder zumindest zählen kann. 

Der Korrelationskoeffizient $Corr(X, Y)$ zwischen den Variablen $X$ und $Y$ ist definiert als 

$$ Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X\cdot\sigma_Y} \in [-1, 1], $$
wobei $Cov(X, Y)$ die *Kovarianz* zwischen den Variablen ist. Eine anschauliche Beschreibung der Korrelation findet sich in @Sauer2019 und in nahezu jedem Statistiklehrbuch.

In R gibt es mehrere Möglichkeiten der Berechnung des Korrelationskoeffizienten nach Pearson, u.a. die Standardfunktion `cor` und die hier verwendete Funktion `correlate` aus dem `corrr`-Paket:

```{r}
library(corrr)

(corr_mat <- raw %>% dplyr::select(Gr.Liv.Area, Lot.Area) %>% correlate())
```

Wenn wir die Korrelation zwischen vielen Variablenpaaren auswerten wollen, bietet das Paket `ggcorrplot` schöne Unterstützung:
```{r fig.width = 10}
library(ggcorrplot)

corr_mat <- raw %>% select_at(all_of(numeric_cols)) %>% 
    dplyr::select(-Order) %>% drop_na() %>% 
    cor()

corr_mat %>% ggcorrplot(type = "upper", insig = "blank", 
                        hc.order = TRUE, 
                        tl.cex = 8, tl.srt = 60)

```

Zu beachten ist hier, dass die Beobachtungen mit fehlenden Werten entfernt werden mussten (mittels `drop_na`), da ansonsten keine Korrelation hätte berechnet werden können. 

Wir können anhand des Diagramms auch schon erkennen, dass es einige (aber nicht viele) Variablen gibt, die stärker (positiv) mit dem Verkaufspreis korreliert sind.




# Datenvorverarbeitung

## Transformationen

### Behandlung kategorieller Variablen

one-hot-encoding oder dummy-encoding

## Variablen entfernen

## Instanzen entfernen



# Modellbildung

Lasso-Modell: Gleichzeitig Features auswählen und Modell fitten

## Trainings- und Testdaten

## Modell erstellen

# Modellinterpretation






Aus der Dokumentation zu dem Paket, die man mittels
```{r}
??AmesHousing
```

aufrufen kann, erfahren wir, dass wir mittels der Funktion `make_ames` eine schon etwas aufbereitete Datenbasis erhalten können:

```{r}
data <- make_ames()
```





## Notizen

Data Doku: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt

GeoLokationen im Artikel http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html

Beispielanalyse: http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html






## Colinearity

* Check for colinearity and correlation, because otherwise the choice of reference level will fail

## Reference level for categorical variables

* Choice of reference levels: Modal factor is level 0
* This independent choice of reference levels per variable only works if variables are in fact independent.



