---
title: "Vorlesung Data Science Praxis"
author: "Andreas Cardeneo"
date: "`r Sys.Date()`"
output: learnr::tutorial
runtime: shiny_prerendered
bibliography: ["citation.bib"]
biblio-style: "apalike"
link-citations: true
---
# Data Science Praxis 

## Einführung

In dieser Vorlesung geht es um Techniken, um *valide* Modelle zu erstellen. Valide Modelle erfüllen die Voraussetzungen der Methoden, die für ihre Analyse verwendet werden. Wir werden hier zunächst mit Lineare Regression arbeiten, weil diese Methode gut verständlich und theoretisch fundiert ist und ihre Ergebnisse gut *erklärbar* sind. Das heißt nicht, dass diese Method auch immer *geeignet* ist, aber in vielen praktischen Fällen kann sie ein guter erster Ansatz sein.

Wir werden einen im Internet und für R verfügbaren Datensatz benutzen, den *Ames Iowa Housing" Datensatz (s. @R-AmesHousing). Dabei handelt es sich um Verkaufsdaten zu Wohnungen und Gebäuden sowie um beschreibende Daten der Immobilien. Es handelt sich damit um ein durchaus anwendungsnahes Szenario.

## Das Szenario

Die *HaiBank AG* möchte im Niedrigzinsumfeld und angesichts steigender Immobilienpreise ihr eigenes Angebot als Immobilienmakler ausbauen. Unter der Marke *ImmoHai* sollen Maklerdienste für Immobilien angeboten werden und eine wichtige Komponente dabei ist die Abschätzung der erwarteten Profitabilität der Vermittlung einer Immobilie. Neben dem eigenen Aufwand auf der Kostenseite ist dafür der erzielbare Verkaufspreis maßgeblich. Traditionell wurde dieser Verkaufspreis von Immobilienexperten nach einer Begehung des Objekts und anhand der Erfahrung eingeschätzt. Weil die Unternehmensstrategie einen deutlichen Ausbau des Maklergeschäfts anstrebt, ist dieser Ansatz aufgrund des hohen Aufwands nicht mehr zu leisten. Stattdessen soll es, ähnlich wie bei der Bonitätsprüfung für Hauskredite, eine Software geben, die anhand von Kennzahlen der Immobilie einen realisierbaren Verkaufspreis prognostiziert. Diese Software soll von Sachbearbeitern im Kundengespräch verwendet werden und dabei helfen zu entscheiden, ob die *HaiBank* als Makler für die betreffende Immobilie auftreten soll.

### Aufgabenstellung

Ihre Aufgabe besteht darin, ein **Prognosemodell für Immobilienpreise** und einen Prototyp für die zugehörige **Anwendung** zu entwickeln, die dieses Prognosemodell verwendet.

Weil Ihre hauseigene IT auf absehbare Zeit noch damit beschäftigt ist, die bislang getrennt in jeder Filiale in Excel-Dateien festgehaltenen Immobiliengeschäfte in eine zentrale Datenbank zu überführen, haben Sie sich aus dem Internet eine frei verfügbare Datensammluung von Immobilienpreisen besorgt. Mit Hilfe dieser Daten können Sie schon einmal den ganzen Prozess der Modellentwicklung durchlaufen und einen Prototypen erstellen, damit Ihr Management auch nachvollziegen kann, wie die fertige Lösung aussehen wird.

Als technische Plattform stehen Ihnen 

* die Programmiersprache *R*, 
* die Entwicklungsumgebung *RStudio* und 
* das auf *R* basierende Framework *flexdashboard* 

für die Entwicklung von Modell und Anwendung zur Verfügung.

### Setup

Um mit *R* zu starten bietet es sich an, ein Konto bei *RStudio Cloud* anzulegen. Damit haben Sie eine *RStudio* Entwicklungsumgebung, die Sie über Ihren Browser nutzen können und eine Basisinstallation von *R* als Ausgangsbasis. Im Rahmen dieses Kurses können Sie einem bereits bestehenden *Workspace* beitreten (Anleitung s. unten). 

Alternativ können Sie *R* und *RStudio* auch lokal auf Ihrem Rechner installieren. Wie dies gemacht wird, erfahren Sie unter [R und RStudio installieren](https://rstudio.com/products/rstudio/download/).

#### RStudio Cloud Workspace
Hier die Erklärung, wie das funktioniert

### Grundsätzliches zur Arbeit mit *R*

Neben Python ist *R* eine der derzeit populärsten Sprachen für Datenanalye, Statistik und maschinellem Lernen. Während Python als vielseitige Programmiersprache gerne von einer Community eingesetzt wird, die aus der Softwareentwicklung kommt, hat *R* seinen Ursprung in der statistischen und wissenschaftlichen Community und bietet sehr vielfältige Möglichkeiten für (explorative) Datenanalyse und statistische Methoden. Zu den letzteren kann man im weiteren Sinne auch Methoden des maschinellen Lernens zählen.

## Lösungskonzeption

wie soll die Lösung am Ende aussehen? Wie kommt man dahin?
welche Daten wird man brauchen? Welche Visualisierungen?
Haben wir Hypothesen?

## Datenzugang

### Datenschutz DSGVO

Rechtsgrundlage und Verarbeitungszweck
Anonymisierung und Pseudonymisierung


## Explorative Datenanalyse

Wenn man eine neue Datenquelle erschließen möchte, bestehen die ersten Schritte aus der *explorativen Datenanalye*. Hierbei geht es darum, ein Verständnis für die Daten zu bekommen und insbesondere Fehler und Auffälligkeiten zu entdecken.

Wir laden zunächst die Immobiliendaten über ein *R*-Paket:
```{r}
library(AmesHousing)
```

Die Daten werden wir unter der Bezeichnung `raw` weiter untersuchen:
```{r}
raw <- ames_raw
names(raw) <- make.names(names(raw))
```

Die zweite Zeile im obigen Codeblock dient dazu, die Bezeichner der Variablen (Spalten des Datensatzes) so umzuwandeln, dass keine Leerzeichen mehr enthalten sind, die uns sonst an späterer Stelle stören würden.

Um uns einen ersten Eindruck von den Daten zu verschaffen, benutzen wir die Funktion `glimpse` aus dem `dplyr`-Paket. Dieses Paket ist grundsätzlich eine gute Wahl für die Datenanalyse, weil es zahlreiche recht intuitiv anwendbare Funktionen für Datentransformationen, Datenfilterung und -aggregation bietet.

```{r}
library(dplyr)

glimpse(raw)
```

Aus dieser Übersicht erfahren wir schon eine ganze Menge:

* die Zahl der Datensätze
* die Zahl der Merkmale
* die Namen der Merkmale 
* die Datentypen der Merkmale
* eine Vorschau auf die Merksmalsausprägungen.

    **Aufgabe**
    Was bedeuten die Ausprägungen *NA* für die Variable *Alley*. 


    **Aufgabe**
    Was bedeutet die Typkennzeichnung `fct` in der Ausgabe von `glimpse`? Was unterscheidet eine *kategorielle* Variable von numerischen? Was wäre der Unterschied, wenn die Variable *Heating* als Zeichenkette (`char` bzw. `character`) abgebildet würde?
    
### Erstes Verständnis der Daten

Mit `glimpse` konnten wir einen ersten Eindruck der Daten gewinnen. Die zahlreichen Einzelwerte bieten aber noch keinen guten Überblick. Dafür ist eine stärker aggregierte Darstellung besser geeignet. Das Paket `dlookr` hilft dabei:
```{r}
library(dlookr)
```

Angewandt auf unsere Immobiliendaten erhalten wir:
```{r}
diagnose(raw)
```

        **Aufgabe**
        * Was können Sie aus der *unique_rate* von 1 für *PID* ableiten? 
        * Was sagt Ihnen die *unique_rate* von 16 für die Variable *MS.SubClass*?
        
#### Zusammenfassung numerischer Daten

Wir können zu numerischen Werten mittels der Funktion `diagnose_numeric` mehr Informationen zu den Variablen erhalten, denen ein numerischer Datentyp zu Grunde liegt:

```{r}
diagnose_numeric(raw)
```

#### Zusammenfassung kategorieller Daten

Für kategorielle Variablen lassen sich keine sinnvollen Quantile und auch kein Mittelwert angeben. Hier sieht die Zusammenfassung für die Variable *MS.Zoning* folgendermaßen aus:
```{r}
diagnose_category(raw, MS.Zoning)
```

*R* bietet übrigens standardmäßig, d.h. ohne weitere Pakete zu laden, die Funktion `summary`, die eine kompakte Übersicht über einen Datensatz bietet:
```{r}
summary(raw)
```

Aber man sieht schon deutlich den Unterschied: Die Ausgabe über die Funktionen des Pakets `dlookr` ist übersichtlicher und auch leichter weiterzuverarbeiten.

### Fehlende Werte

In vielen Datensätzen, die aus unternehmensinternen IT-Systemen bezogen werden, finden sich Datensätze mit Merkmalen ohne Ausprägung. Es gibt keine pauschale Vorgehensweise, die bzgl. der Behandlung dieser fehlenden Werte immer richtig ist. Zunächst kann es sich lohnen die Spalten (Variablen) in den Datensätzen zu identifizieren, die viele fehlende Werte aufweisen.

Gehen wir nochmal zurück zur obigen Diagnose:
```{r}
diagnose(raw)
```

Die Spalte *missing_percent* gibt den Anteil fehlender Werte an und wir können die Spalten mit häufig fehlenden Werten über eine Sortierung erkennen:
```{r}
diagnose(raw) %>% arrange(desc(missing_percent))
```

Deutlich wird, dass die Variable *Pool.QC* wohl häufig keine Werte enthält. Eine Vermutung wäre, dass diese Variable etwas mit dem Vorhandensein eines Pools zu tun hat und dass fehlende Werte schlichtweg bedeuten, dass die Immobilie keinen Pool hat. Etwas anders sieht es bei der Variable *Bsmt.Cond* aus. Diese fehlt in knapp 3% aller Fälle -- ebenso wie andere Variablen mit ähnlichem Namen. Vielleicht ist es hier ähnlich wie bei der Variablen *Pool.QC* und nicht vorhandene Werte bedeuten einen fehlenden Keller. Aber kann es sein, dass bei ca. 3% aller Immobilien kein Keller vorhanden ist? Um dies besser zu verstehen, müsste man mehr über die Bauweise in der Region in Erfahrung bringen: Vielleicht wurden früher bei der Erfassung der Immobiliendaten keine Angaben zum Keller aufgenommen und die Daten fehlen schlichtweg für ältere Immobilien?

Unabhängig davon, was hier im konkreten Fall Ursache für die fehlenden Werte ist: Bei der explorativen Datenanalyse müssen wir stets berücksichtigen, dass das Fehlen selbst auch eine fachliche Information (hier: *kein Pool vorhanden*) bedeuten kann und nicht immer auf Fehler bei der Datenaufnahme oder -übertragung zurückzuführen ist bzw. mit Änderungen in der Erfassungssystematik zu tun haben muss.

Wir haben durch die obige Analyse schon den Verdacht, dass fehlende Werte für mehrere Variablen in typischen Kombinationen vorkommen: Wenn es keinen Keller gibt oder keine Werte erfasst wurden, dann fehlen wahrscheinlich Werte für alle diesbezüglichen Variablen. Um das zu untersuchen, können wir das Paket `VIM` verwenden:

```{r}
library(VIM)
```

Wenden wir es auf unsere Daten an, wobei wir uns auf die Variablen beschränken, die laut vorangehender Analyse fehlende Werte aufweisen:
```{r}
missing_cols <- diagnose(raw) %>% filter(missing_percent > 0.0) %>% pull(variables)

raw %>% select_at(vars(all_of(missing_cols))) %>% aggr(combined = TRUE, sortVars = TRUE, cex.axis = 0.5)
```

Die für diese Untersuchung gewählte Beschränkung auf Variablen, die fehlende Werte aufweisen, birgt allerdings ein Problem: Auf diese Weise entgehen uns Zusammenhänge zwischen fehlenden Werten und anderen Variablen, die möglicherweise auf einen systematischen Zusammenhang schließen lassen. Vielleicht fehlen ja die Angaben zu Kellern für Gebäude bestimmter Jahrgänge oder aus bestimmten Gebieten, deren geologischen Eigenschaften keinen Kellerbau zulassen?

Gehen wir einem konkreten Verdacht nach, dass fehlende Werte für die *Bsmt*-Variablen mit der Lage der Immobilie zu tun haben. Hierfür können wir die Variable *Neighborhood* und die Variable *Bsmt.Cond* verwenden:

```{r}
library(ggplot2)

raw %>% dplyr::select(Neighborhood, Bsmt.Cond) %>% 
    mutate(has_basement = !is.na(Bsmt.Cond)) %>% 
    count(Neighborhood, has_basement) %>% 
    ggplot(aes(x = Neighborhood, y = n, fill = has_basement)) +
    geom_col(position = "fill") + 
    coord_flip() +
    scale_y_continuous(labels = scales::percent)

```

Anscheinend ist der Verdacht eines geographischen Zusammenhangs nicht unbegründet und das hilft uns, die Daten besser zu verstehen.

#### Systematisierung der Ursachen fehlender Werte

In @Sauer2019 (und auch an anderen Stellen) werden die Ursachen für fehlende Werte systematisch beschrieben. Im wesentlichen werden drei verschiedene Fälle unterschieden:

* Missing completely at random (MCAR),
* Missing at random (MAR),
* Missing not at random (MNAR)

Der erste Fall (MCAR) liegt vor, wenn keine bekannte bzw. erkennbare Ursache für das Fehlen vorliegen, d.h. das Fehlen ist insbesondere nicht auf die Ausprägungen anderer Variablen für dieselbe Instanz (dieselbe Beobachtung) zurückzuführen. Typische Ursachen hierfür wären fehlende Daten aufgrund von technischen Übermittlungsfehlern, die nichts mit der Beobachtung selbst zu tun haben. 

Das andere Extrem in dieser Einteilung ist die Klasse MNAR: Hier hängt die Wahrscheinlichkeit für das Fehlen der Ausprägung einer Variablen für eine Beobachtung von der Ausprägung selbst ab. Die Variable *Lot.Frontage* (Bedeutung *Linear feet of street connected to property*) aus unserem Immobiliendatensatz würde hier passen: Die Ausprägung fehlt im Datensatz, sobald die wahre Ausprägung 0 lautet, weil das Grundstück nicht an eine Straße angrenzt.

Dazwischen ist die Klasse MAR. Die Ursache für fehlende Werte liegt hier nicht in der Ausprägung der Variablen selbst, sondern hängt mit anderen Variablen zusammen. Die Variable *Garage.Yr.Blt* (Bedeutung: Baujahr der Garage) fehlt bspw. dann, wenn es keine Garage gibt. Das Fehlen liegt aber nicht am Baujahr selbst.

Die Systematisierung ist kein Selbstzweck. Wenn wir wissen, dass der MNAR-Fall vorliegt, sollten wir über eine neue Variable, die wir selber einführen, unser Wissen über die Gründe für das Fehlen explizit in unseren Datensatz aufnehmen. Im Beispiel könnten wir eine kategorielle Variable *street_access* mit den Werten 0 für *keinen Straßenzugang* und 1 für *mit Straßenzugang* einführen. 
Im Fall von MCAR kann man die fehlenden Werte evtl. gezielt ergänzen. Dieses Vorgehen wird *Imputation* genannt und es gibt verschiedene Verfahren dafür. Andererseits kann man die entsprechenden Instanzen evtl. auch löschen, d.h. für die weitere Betrachtung ignorieren, weil ihr Fehlen keine statistischen Verzerrungen einführt. 
Bei fehlenden Werten nach MAR würde das Löschen von Instanzen allerdings zu statistischen Verzerrungen führen. Im Beispiel hier würde man alle Immobilien ohne Garage entfernen, wenn man die Instanzen löscht, für die die Variable *Garage.Yr.Blt* keine Ausprägung hat. Damit hätte man kein repräsentatives Bild des Immobilienmarkts mehr.

##### Fehlende Werte in der Praxis

In der Praxis, gerade bei der Verwendung von Daten aus operativen IT-Systemen, wird das Fehlen von Werten oftmals durch besondere Ausprägungen codiert. Ein Grund dafür ist, dass die Benutzeroberfläche der IT-Systeme Eingaben in den entsprechenden Feldern erwartet, die der oder die SachbearbeiterIn im Moment der Eingabe nicht leisten kann. In numerischen Feldern wird dann gerne ein Wert wie 99 oder 999 eingetragen, in Textfeldern wird ein Leerzeichen "⍽", ein "-" oder "." eingetragen. 

#### Umgang mit fehlenden Werten

Wie auch in @Sauer2019 dargestellt wird, ist der richtige Umgang mit fehlenden Werten ein komplexes Thema. Simple Lösungen bestehen darin, die fehlenden Werte durch den Mittelwert der (numerischen) Ausprägungen der Variable zu ersetzen. Bei kategoriellen Variablen kann man analog dazu den Modalwert, d.h. die häufigste Ausprägung, als Ersatzwert verwenden. Aber beide Vorgehen sind stark vereinfachend und können zu unrealistischen Merkmalskombinationen für die betroffenen Beobachtungen führen. Technisch kann so eine Ersetzung in R so umgesetzt werden:
```{r}
library(tidyr)

completed <- raw %>% mutate_if(is.numeric, ~ replace_na(., mean(., na.rm = TRUE)))
```

Die Funktion `replace_na` aus dem `tidyr`-Paket ersetzt *NA*-Werte durch den im zweiten Argument angegebenen Wert.

Um andere Imputationsstrategien kennenzulernen, wechseln wir zeitweise zu einem anderen Datensatz:
```{r}
library(MASS)
glimpse(whiteside)
```

Der Datensatz passt zum Immobilienszenario, weil er den Zusammenhang zwischen Gasverbrauch, Außentemperatur und Gebäudeisolierung zeigt.

```{r}
ggplot(whiteside, aes(x = Temp, y = Gas)) + geom_point() + 
    geom_smooth(method = "lm") + 
    facet_wrap(~ Insul)
```

Für das Ausprobieren entfernen wir zufällig einen kleinen Teil der Messungen des Gasverbrauchs aus den Daten:

```{r}
set.seed(123)

ws_mod <- whiteside %>% mutate(Gas = if_else(runif(nrow(whiteside)) < 0.15, NA_real_, Gas))
```

Im folgenden probieren wir aus, was passiert, wenn wir die  Imputationsstrategie *Imputation durch Mittelwert* wählen. Dazu verwenden wir das Paket `mice` (@VanBuuren2011).

Die Imputation durch den Mittelwert für alle numerischen Spalten kann durch folgenden Funktionsaufruf erreicht werden:

```{r} 
library(mice)

imp <- ws_mod %>% mice(method = "mean", m = 1, maxit = 1)
```

Das Ergebnis der Imputation können wir mit etwas Aufwand visualisieren (das hier geladene Paket `forcats` bietet Funktionen zur Bearbeitung von kategoriellen Variablen):
```{r}
library(forcats)

tmp1 <- imp$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    arrange(i) %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)

    
```

Aus fachlichen Gründen können wir davon ausgehen, dass es einen Zusammenhang zwischen Temperatur und Gasverbrauch gibt: Die Trendlinien oben legen das auch nahe.

```{r}
imp2 <- ws_mod %>% mice(method = "norm.predict", seed = 1, m = 1, print = FALSE)
```

Das Ergebnis dieser Imputation können wir auch wieder mit dem obigen Code visualisieren:
```{r}
tmp1 <- imp2$imp$Gas %>% 
    transmute(i = as.integer(row.names(.)),
              value = `1`)

tmp2 <- ws_mod %>% 
    mutate(i = row_number()) %>% 
    left_join(tmp1, by = "i") %>% 
    mutate(Gas = if_else(is.na(Gas), value, Gas), 
           imputed = fct_rev(as.factor(!is.na(value))))
    
ggplot(tmp2, aes(x = Temp, y = Gas, color = imputed)) + 
    geom_point() + facet_wrap(~ Insul)
```

Dieses Ergebnis sieht wesentlich plausibler aus als der erste Versuch auf Basis des Mittelwerts. 


### Ausreisser identifizieren

Wir kehren zurück zum *AmesHousing*-Datensatz.

Die obigen Diagnosetabellen bieten konkrete Information und man kann bspw. über den Vergleich des Durchschnitts mit dem Median einen ersten Eindruck der Verteilung der Werte bekommen. Weiterhin wichtig ist die Spalte *outlier*, die anzeigt, wie viele Ausreißer für die jeweilige Variable erkannt wurden. Die Definition, welche Werte als Ausreißer angesehen werden, gleicht der Definition von Ausreißern für *Boxplots* (siehe auch die Dokumentation zur Funktion `boxplot` mittels `?boxplot`).

Tatsächlich ist es sehr hilfreich, Daten und ihre Verteilungen zum besseren Verständnis zu visualisieren. Die Ausreißer der Variable *SalePrice* können wir folgendermaßen untersuchen:

```{r}
plot_outlier(raw, SalePrice)
```

#### Ausreisser behandeln

Wenn man feststellt, dass es sich bei den Ausreissern nicht um Fehler handelt, bspw. Tippfehler oder falsche Sensormeldungen, hat man folgende Möglichkeiten, mit den Ausreissern umzugehen:
* *Trimmen*: Beim Trimmen gibt man einen Wert $p\in[0,1]$ vor, bspw. $p = 0.9$, und schließt dann alle Beobachtungen aus, bei denen die betrachtete Variable einen Wert unterhalb des $100\cdot\frac{p}{2}\%$-Perzentils bzw. oberhalb des $100\cdot(1 - \frac{p}{2})\%$-Perzentils hat. Im Beispiel würde man also alle Beobachtungen mit Werten unterhalb des 5%- bzw. oberhalb des 95%-Perzentils entfernen.
* *Winsorisieren*: Hier gibt man auch einen Wert $p\in[0,1]$ vor, bspw. $p = 0.9$, und setzt dann alle Datenwerte unterhalb des $100\cdot\frac{p}{2}\%$-Perzentils auf den Wert des $100\cdot\frac{p}{2}\%$-Perzentils und analog dazu alle Werte größer als das $100\cdot(1 - \frac{p}{2})\%$-Perzentil auf den Wert des $100\cdot(1 - \frac{p}{2})\%$-Perzentils. Im Beispiel wären die untere bzw. obere Grenze also das 5%- bzw. das 95%-Perzentil.

Beim Winsorisieren verliert man also keine Datensätze, verändert die vorhandenen aber. 

### Seltene Werte

Wie oben erwähnt, können kategorielle Variablen nicht gut numerisch zusammengefasst werden. Die Verteilung der Merkmalsausprägungen ist hingegen sehr wertvoll, weil schnell die häufigen und seltenen Ausprägungen erfasst werden können. Ein einfaches *Histogramm* lässt sich mit dem Paket `ggplot2` folgendermaßen erstellen:
```{r}
raw %>% ggplot(aes(x = MS.Zoning)) + geom_histogram(stat = "count")
```

Aus Histogrammen können wir ableiten, welche Werte je Variable sehr selten auftreten. Gerade bei der Analyse kategorieller Variablen stellt sich oft heraus, dass einige Ausprägungen sehr selten auftreten. Eine statistische Analyse auf Basis dieser Ausprägungen würde statistisch wertlos sein, da die Anzahl der Beobachtungen pro Ausprägung nicht für statistisch valide Aussagen ausreicht. Daher ist es empfehlenswert und üblich, seltene Ausprägungen zu Gruppen zusammenzufassen und eine neue Dummy-Ausprägung, bspw. *sonstige*, für diese einzuführen.

Im Folgenden untersuchen wir die kategoriellen Variablen des *AmesHousing*-Datensatzes. Um eine vollständige Liste der kategoriellen Variablen zu erhalten, hilft die [Dokumentation des AmesHousing-Datensatzes](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). Übrigens finden sich auf der Seite [Gimme Shelter](https://www.gimme-shelter.com/category/term-2/) recht gute Erklärungen einiger (amerikanischen) Fachbegriffe der Immobilienbranche. 

In der Datendokumentation sehen wir, dass kategorielle Variablen als *Nominal* oder *Ordinal* aufgeführt werden. Wir könnten jetzt manuell eine Liste anlegen, aber als Data Scientists machen wir das natürlich nicht, sondern extrahieren die notwendige Information als Datensatz (die Basis für den hier verwendeten Code kommt von [Ron Sarafian](http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html)).

```{r}
library(stringr)
library(purrr)

housing_variable_types <- function(doc_url = "http://jse.amstat.org/v19n3/decock/DataDocumentation.txt") {
  doku <- url(doc_url, encoding = "latin1") %>% readLines() 
  data_type_lines <- grep("Nominal|Ordinal|Discrete|Continuous", doku, value = TRUE)

  data_type_lines %>% str_match_all("^(.+)( |\\s)\\((.+)\\)(\\s*):") %>% compact %>% 
    map(~ c(VAR = .[,2], TYPE = .[,4])) %>% map_dfr(as.list) %>% 
    mutate(VAR = make.names(VAR))
}

type_data <- housing_variable_types()
```

Jetzt liegen die Spaltenbezeichnungen zusammen mit der Datenart als `data.frame` für die weitere Verarbeitung vor. Leider stellt sich heraus, dass die Dokumentation in einigen Fällen von den Spaltenbezeichnung im Datensatz abweicht, so dass wir diese Bezeichnungen manuell korrigieren müssen. Ein schönes Beispiel dafür, dass man bei der explorativen Datenanalyse immer wieder unerwartet auf Probleme stößt.

```{r}
housing_categorials <- function(type_frame) {
  type_frame %>% filter(TYPE %in% c("Nominal", "Ordinal")) %>% dplyr::select(VAR) %>% 
    mutate(VAR = case_when(
        VAR == "Exterior.1" ~ "Exterior.1st",
        VAR == "Exterior.2" ~ "Exterior.2nd",
        VAR == "HeatingQC" ~ "Heating.QC",
        VAR == "KitchenQual" ~ "Kitchen.Qual",
        VAR == "FireplaceQu" ~ "Fireplace.Qu",
        VAR == "BsmtFinType.2" ~ "BsmtFin.Type.2",
        TRUE ~ VAR
    )) %>% pull()
}

categorials <- housing_categorials(type_data)

```

Zunächst modifizieren wir unseren Datensatz so, dass R die kategoriellen Variablen erkennt, indem wir die entsprechenden Spalten zu `factors` konvertieren.
```{r}
raw <- raw %>% mutate_at(vars(all_of(categorials)), as.factor) 
```


Jetzt können wir uns ansehen, wieviele verschiedene Ausprägungen wir für die kategoriellen Variablen haben. Nachdem wir schon wissen, dass *PID* eine eindeutige Kennung ist, lassen wir diese Variable aus der Betrachtung heraus. Für die graphische Darstellung bringen wir die Daten mit der `pivot_longer`-Funktion in ein *langes Format* (s. auch @Sauer2019), bei dem die Variablen nicht mehr einzeln in Spalten stehen. Zum besseren Verständis wird die Bildung des langen Datensatzes auf zwei Schritte mit zwischenzeitlicher Ausgabe der Tabelle aufgeteilt.

```{r} 
categ_freq <- raw %>% dplyr::select(all_of(categorials)) %>% dplyr::select(-PID) %>% 
    summarise_all(~ length(unique(.))) 

head(categ_freq)

categ_freq <- categ_freq %>% pivot_longer(cols = everything(), names_to = "Variable", values_to = "Werte")

head(categ_freq)

categ_freq %>% 
    ggplot(aes(x = reorder(Variable, Werte), y = Werte)) + geom_col() + coord_flip()
```

Im nächsten Schritt gucken wir uns alle kategoriellen Variablen an, die 6 oder mehr verschiedene Ausprägungen haben. Für diese erstellen wir jeweils ein Histogramm der Ausprägungen, um zu sehen, ob es seltene Ausprägungen gibt.
```{r}
rel_categs <- categ_freq %>% filter(Werte >= 6) %>% pull(Variable)

categ_levels <- raw %>% dplyr::select(all_of(rel_categs)) %>% map(~ fct_count(.))

```

Die Informationen über die Häufigkeit visualisieren wir: Weil alle unsere Plots verschiedene Ausprägungen haben und unterschiedlich viele Ausprägungen, bietet sich der üblich Weg in `ggplot` über `facet_wrap` oder `facet_grid` nicht an. Hier ist das Paket `patchwork` hilfreich, durch das wir eine Liste von Plots automatisch in einer Graphik anordnen können.
```{r fig.height = 24}
library(patchwork)

plot_level_freq <- function(idx, data) {
    data[[idx]] %>% ggplot(aes(x = reorder(f, n), y = n)) + geom_col() + coord_flip() + xlab(names(data)[[idx]])
}

var_cl_plots <- lapply(seq_along(categ_levels), plot_level_freq, categ_levels)

wrap_plots(var_cl_plots, ncol = 2)
```

Zwar sind nicht alle Plots gut lesbar, aber wir sehen, was wir wissen wollten: Es gibt für viele kategorielle Variablen Gruppen von Ausprägungen, die sehr selten vorkommen.
Weiter unten bei der Datenvorverarbeitung für die Modellierung, werden wir daher solche kategoriellen Variablen entsprechend bearbeiten und seltene Ausprägungen zusammenfassen.

## Informationsbeiträge von Variablen

Nachdem wir uns im vorherigen Abschnitt mit kategoriellen Variablen befasst haben, kehren wir hier wieder zurück zu den numerischen Variablen. Wir sollten uns in Erinnerung rufen, das wir letztlich ein Modell für den Verkaufspreis von Immobilien entwickeln wollen. Dahinter steckt die Annahme, dass verschiedene Eigenschaften bzw. Eigenschaftskombinationen von Immobilien Einfluss auf den Preis haben. 

### Beitrag einzelner Variablen

Wir können davon ausgehen, dass Eigenschaften, die für alle Immobilien gleich oder zumindest sehr ähnlich sind, auf den Preis keinen Einfluss haben werden. Wenn wir also Variablen identifizieren, die wenig informativ sind, können wir diese von der weiteren Betrachtung ausschließen.

Wenn wir nur einzelne Variablen betrachten, können wir als Statistik für den Informationsgehalt numerischer Variablen die Varianz der Ausprägungen (Werte) verwenden. Die Spalte *Order* können wir dabei ignorieren, weil sie einfach nur eine Ordnungszahl enthält.

Als ersten Schritt visualisieren wir die Verteilungen (genauer: Dichtefunktionen) der numerischen Variablen. Nachdem wir oben schon die kategoriellen Variablen identifiziert haben, können wir die numerischen einfach als Differenz zur vollständigen Variablenmenge ermitteln:

```{r}
library(tidyselect)

numeric_cols <- setdiff(names(raw), categorials)
```

Für die graphische Darstellung müssen wir den Datensatz wieder in ein *langes Format* bringen, analog dem Vorgehen weiter oben. 
```{r fig.height=24}
raw_rs <- raw %>% select_at(all_of(numeric_cols)) %>% dplyr::select(-Order) %>% 
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Wert") 

plot_density <- function(data) {
    data %>% filter(!is.na(Wert)) %>% 
        ggplot(aes(x = Wert)) + geom_density() + xlab(first(data$Variable))
}

var_data <- raw_rs %>% group_split(Variable)

plots <- lapply(var_data, plot_density)

wrap_plots(plots, ncol = 2)
```

Durch die Dichtediagramme können wir schon ahnen, welche Variablen wahrscheinlich eine niedrige Varianz aufweisen. Allerdings kann die Intuition auch täuschen, so dass wir die Varianzen der Variablen auch noch rechnerisch bestimmen:
```{r}
(nvar_var <- raw %>% select_at(all_of(numeric_cols)) %>% dplyr::select(-Order) %>% 
    summarise_all(~ var(., na.rm = TRUE) ) %>%     
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Varianz") %>% 
        arrange(Varianz))
```

Für die Datenvorverarbeitung werden wir Variablen entfernen, die eine niedrige Varianz haben. Umgekehrt können wir in der Analyse auch die Variablen ignorieren, die nur verschiedene Werte aufweisen - hier dürfte es sich fast immer um Datensatzschlüssel handeln.

### Zusammenhang zwischen Variablen: Korrelation

Zwischen metrischen Variablen bestehen oftmals mehr oder weniger stark ausgeprägte lineare Zusammenhänge. Dieser Zusammenhang lässt sich als *Korrelationskoeffizient* (man unterscheidet mehrere verschiedene Korrelationskoeffizienten, s. auch @Sauer2019. Hier verwenden wir den nach *Pearson*). Der Begriff *metrische Variable* bedeutet, dass die Variable intervall- oder verhältnisskaliert ist. Das gilt für stetige numerische Variablen und oftmals auch für diskrete numerische Variablen - im Prinzip immer dann, wenn man die Variable messen oder zumindest zählen kann. 

Der Korrelationskoeffizient $Corr(X, Y)$ zwischen den Variablen $X$ und $Y$ ist definiert als 

$$ Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X\cdot\sigma_Y} \in [-1, 1], $$
wobei $Cov(X, Y)$ die *Kovarianz* zwischen den Variablen ist. Eine anschauliche Beschreibung der Korrelation findet sich in @Sauer2019 und in nahezu jedem Statistiklehrbuch.

In R gibt es mehrere Möglichkeiten der Berechnung des Korrelationskoeffizienten nach Pearson, u.a. die Standardfunktion `cor` und die hier verwendete Funktion `correlate` aus dem `corrr`-Paket:

```{r}
library(corrr)

(corr_mat <- raw %>% dplyr::select(Gr.Liv.Area, Lot.Area) %>% correlate())
```

Wenn wir die Korrelation zwischen vielen Variablenpaaren auswerten wollen, bietet das Paket `ggcorrplot` schöne Unterstützung:
```{r fig.width = 10}
library(ggcorrplot)

corr_mat <- raw %>% select_at(all_of(numeric_cols)) %>% 
    dplyr::select(-Order) %>% drop_na() %>% 
    cor()

corr_mat %>% ggcorrplot(type = "upper", insig = "blank", 
                        hc.order = TRUE, 
                        tl.cex = 8, tl.srt = 60)

```

Zu beachten ist hier, dass die Beobachtungen mit fehlenden Werten entfernt werden mussten (mittels `drop_na`), da ansonsten keine Korrelation hätte berechnet werden können. 

Wir können anhand des Diagramms auch schon erkennen, dass es einige (aber nicht viele) Variablen gibt, die stärker (positiv) mit dem Verkaufspreis korreliert sind.

#### Multikollinearität

Allgemein spricht man von *(Multi)Kollinearität*, wenn zwei oder mehrere unabhängige Variablen stark miteinander korreliert sind (s. @Sheather2009). In verschiedenen Methoden, darunter auch linearer Regression, führt Multikollinearität dazu, dass die Bedeutung einzelner Variablen nicht isoliert bewertet werden kann. Es kann sogar passieren, dass die Regressionskoeffizienten das falsche Vorzeichen haben und damit nicht nachvollziehbare Abhängigkeiten suggerieren (s. @Sheather2009 für ein Beispiel). Um zu ermitteln, wie stark Variablen von Multikollinearität betroffen sind, wird der sog. *Variance Inflaction Factor* (VIF) bestimmt. Die Bezeichung kommt daher, dass man zeigen kann (s. @Sheather2009), dass die Varianz der Regressionskoeffizienten einer Variablen mit der Korrelation zu anderen (unabhängigen) Variablen steigt. Da eine große Varianz der Regressionskoeffizienten für ein valides Modell unerwünscht ist, kann man dann entscheiden, Variablen mit großen VIF nicht in das Modell aufzunehmen.  

In R gibt es (natürlich) verschiedene Möglichkeiten, den VIF-Wert zu bestimmen. Hier verwenden wir die `vif`-Funktion aus dem `car`-Paket. 
Die Funktion erhält als Eingabe ein *lineares Modell* basierend auf einer R *Formel*, die die abhängige Variable zu den unabhängigen in Beziehung setzt. Wir probieren das an dieser Stelle für einige ausgewählte Variablen aus (und orientieren uns dabei an der Graphik zur Korrelation oben).
Zunächst erstellen wir ein lineares Modell mit dem Verkaufspreis al abhängiger Variable und mehreren unabhängigen.

```{r}
library(car)

linmod <- lm(SalePrice ~ Gr.Liv.Area + Wood.Deck.SF + Bedroom.AbvGr + Full.Bath + TotRms.AbvGrd,
             data = raw %>% select_at(all_of(numeric_cols)))

summary(linmod)

```

Der Koeffizient für die Variable *Bedroom.AbvGr* ist negativ, was erstmal erstaunlich ist, denn das würde bedeuten, dass der Verkaufspreis mit mehr Schlafzimmern sinkt. Gucken wir uns jetzt die *VIF* an:
```{r}
vif(linmod)
```

Der VIF-Wert für eine unabhängige Variable wird über das *Bestimmtheitsmaß* $R^2$ des linearen Regressionsmodells auf den verbleibenden unabhängigen Variablen berechnet: 

```{r}
bdrm_mod <- lm(formula = Bedroom.AbvGr ~ Gr.Liv.Area + Wood.Deck.SF + Full.Bath + TotRms.AbvGrd, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(bdrm_mod)
```

Der VIF-Wert für die Variable *Bedroom.AbvGr* wird dann berechnet als
$$ VIF(\text{Bedroom.AbvGr}) = \frac{1}{1 - R_{\text{Bedroom.AbvGr}}^2} = \frac{1}{1 - 0.4597} = 1.851.$$
Analog wird der VIF-Wert für die Variable *Gr.Liv.Area* ermittelt:

```{r}
gla_mod <- lm(formula = Gr.Liv.Area ~ Bedroom.AbvGr + Wood.Deck.SF + Full.Bath + TotRms.AbvGrd, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(gla_mod)
```
$$ VIF(\text{Gr.Liv.Area}) = \frac{1}{1 - R_{\text{Gr.Liv.Area}}^2} = \frac{1}{1 - 0.7205} = 3.578.$$

Die Frage ist nun, ob solche VIF-Werte bedenklich sind. Diese Frage ist nich so einfach zu beantworten. Klar ist, dass der VIF-Wert mindestens 1 beträgt und gegen unendlich gehen kann. Bei einem Bestimmtheitsmaß $R^2$ von 0.64, d.h. einem *Korrelationskoeffizienten* von 0.8 (also schon recht stark korreliert), würde man einen VIF-Wert 
$$ VIF = \frac{1}{1 - 0.64} = 2.78 $$ erhalten. In der Literatur (u.a. auch in @Sheather2009), wird oft ein VIF von mindestens 5 als kritisch gesehen. Das würde einem (betragsmäßigem) Korrelationskoeffizienten von 
$$ R = \sqrt{1 - \frac{1}{VIF}} = \sqrt{1 - \frac{1}{5}} = 0.89$$ 
entsprechen, also knapp 0.9. Wir sehen, der VIF-Wert wächst recht schnell für kleinere Änderungen des Korrelationskoeffizienten und es gibt auch Quellen, z.B. https://statisticalhorizons.com/multicollinearity, die ab einem VIF von 2.5 vorsichtig werden. 

Wie würde das Regressionsmodell aussehen, wenn wir nur die hochkorrelierte Variable *Gr.Liv.Area* verwenden würden?
```{r}
gla1_mod <- lm(formula = SalePrice ~ Gr.Liv.Area, 
               data = raw %>% select_at(all_of(numeric_cols)))

summary(gla1_mod)
```

Und wenn wir die anderen nicht hoch korrelierten Variablen nehmen?

```{r}
lc_mod <- lm(formula = SalePrice ~ Wood.Deck.SF + Bedroom.AbvGr + Full.Bath, 
             data = raw %>% select_at(all_of(numeric_cols)))

summary(lc_mod)
```

Also offensichtlich können wir nicht einfach alle hochkorrelierten Variablen einfach entfernen, ohne die Modellqualität (hier gemessen anhand des Bestimmtheitsmaßes) deutlich zu verschlechtern. Der VIF-Wert gibt uns leider nicht an, woher genau die hohe Korrelation stammt, aber aus der obigen Korrelationsmatrix können wir sehen, dass *TotRms.AbvGrd* stark mit den anderen Variablen korreliert ist. Daher prüfen wir ein Modell ohne diese Variable:

```{r}
gwbf_mod <- lm(formula = SalePrice ~ Gr.Liv.Area + Wood.Deck.SF + Bedroom.AbvGr + Full.Bath, 
             data = raw %>% select_at(all_of(numeric_cols)))

summary(gwbf_mod)

vif(gwbf_mod)
```

Damit erreichen wir die Modellqualität des ersten Modells und senken die VIF-Werte, d.h. wir haben ein einfacheres Modell gleicher Qualität erreicht, das besser erklärbar und robuster ist (weniger Varianz in den Regressionskoeffizienten). Aber der Koeffizient für die Schlafzimmerzahl ist immer noch negativ...

## Datenvorverarbeitung

Nachdem wir ein besseres Verständnis für die Daten und die Zusammenhänge zwischen den Variablen erhalten haben, können und müssen wir vor der Modellerstellung noch weitere Vorverarbeitungsschritte vornehmen. Tatsächlich kann dieser Schritt nicht ohne Verständnis für das angestrebte Modell durchgeführt werden: Einige Arten von Modellen können bspw. nicht mit fehlenden Werten oder kategoriellen Variablen umgehen (beides gilt für lineare und logistische Regression)[^1]. Nachdem man zu Beginn oftmals noch nicht sicher weiß, welche Art von Modell das beste Ergebnis liefert, ist die Datenvorbereitung in der Praxis ein iterativer Prozess. 

Wir werden uns, wie oben auch schon praktiziert, hier mit einem linearen Regressionsmodell beschäftigen. Unser Modell wird also die Form
$$ \hat{y} = \beta_0 + \sum_{i=1}^n \beta_i\cdot x_i + \varepsilon$$ haben.
Dabei ist $\hat{y}$ die Schätzung für die *Zielvariable* (hier der Verkaufspreis), die $x_i$ sind $n$ unabhängige Variablen (die Prädiktoren) und $\varepsilon$ ist ein Fehlerterm. 
Gesucht sind die Koeffizienten $\beta_0$ (konstanter Term, *intercept*) und $\beta_i$. Ermittelt werden diese Koeffizienten anhand von $N$ *Trainingsdatensätzen* $(y^j, x_1^j, x_2^j,\ldots,x_n^j), j = 1,\ldots,N$ durch Lösen des Optimierungsproblems 
$$ \min_{\beta_0, \beta_1,\ldots,\beta_n} \sum_{j=1}^N \left(y^j - \left(\beta_0 + \sum_{i=1}^n \beta_i\cdot x_i^j\right)\right)^2. $$
Für dieses Optimierungsproblem kann man geschlossene Ausdrücke für die optimalen Koeffizienten $\beta_0^*, \beta_1^*,\ldots, \beta_n^*$ angeben und die Lösung somit effizient berechnen (s. @Sheather2009).

[^1]: Hier darf man nicht das Verfahren an sich mit der Implementierung des Verfahrens verwechseln. Diese kann bspw. automatisch Datensätze mit fehlenden Werten entfernen und dann wirkt das Verfahren nach außen hin so, als könnte es mit fehlenden Werten umgehen. 

Für die Datenvorbereitung des Modells werden wir das `recipes`-Paket verwenden. Dieses Paket bietet schon eine Vielzahl typischer Vorbereitungsschritte, die sich einfach zu einer *Pipeline* zusammensetzen lassen. Diese Pipeline kann dann auch einfach wieder auf andere Daten angewandt werden, was die Reproduzierbarkeit und Wiederverwendung erleichtert.

Wir beginnen das *Rezept* mit der Aufteilung des Datensatzes in einen Trainings- und einen Testdatensatz. Auf Basis des Trainingsdatensatzes wird das Modell *gelernt* und die Güte des gelernten Modells wird dann anhand des Testdatensatzes gemessen. Damit ist sichergestellt, dass wir messen, wie gut das Modell die wesentlichen Eigenschaften der Daten abbildet und generalisieren kann: Schließlich sind wir daran interessiert, eine Verkaufspreisvorhersage für zuvor ungesehene Daten zu erhalten.

Damit wir aber sicher sind, dass wir auch auf den richtigen Daten arbeiten, wiederholen wir an dieser Stelle ein paar der Erstellungsschritte von oben:

### Grundlage (wieder)herstellen

```{r}
housing_data <- ames_raw
names(housing_data) <- make.names(names(housing_data))

type_data <- housing_variable_types()
categorials <- housing_categorials(type_data)

housing_data <- housing_data %>% mutate_at(vars(all_of(categorials)), as.factor) 

```

### Rezept anlegen
```{r}
library(rsample)

set.seed(123456)

data_split <- initial_split(housing_data)

training_data <- training(data_split)
test_data <- testing(data_split)
```

Das Rezept wird angelegt, indem wir die Zielvariable angeben und pauschal alle anderen Variablen als potentielle Prädiktoren vorsehen. Im weiteren Verlauf werden wir das Rezept durch die Verkettung mit `step_*`-Funktionen weiter spezifizieren. Viele der `step_*`-Funktionen aus dem `recipes`-Paket bieten noch Parameter, um das Verhalten weiter zu beeinflussen. Die Dokumentation zum `recipes`-Paket listet unter https://recipes.tidymodels.org/articles/Ordering.html auch eine empfohlene Reihenfolge der Schritte auf.

```{r}
library(recipes)

housing_rec <- recipe(SalePrice ~ ., data = training_data)
```

### Imputation fehlender Werte
Zunächst werden wir fehlende Werte mit einer komplexeren Imputationsstrategie, nämlich *kNN-Imputation* (s. u.a. @Beretta2016), durch plausible Werte ersetzen:
```{r}
housing_rec %>% 
  step_knnimpute(all_predictors(), neighbors = 3) -> housing_rec
```

Um noch etwas deutlicher den *Fluss* der Transformationen darzustellen, haben wir hier den Zuweisungsoperator in der Form `->` verwendet.

### Seltene Kategorien zusammenfassen

Wir haben oben gesehen, dass es einige kategorielle Variablen gibt, die sehr seltene Ausprägungen aufweisen. Diese fassen wir zu Gruppen zusammen:
```{r}
housing_rec %>% 
  step_other(all_nominals(), threshold = 0.05) -> housing_rec
```

### Konstante Variablen entfernen

Der nächste Schritt entfernt die Variablen, die entweder eine sehr kleine Varianz oder eine sehr stark dominierende Ausprägung haben, d.h. wir wenden die Funktion `step_nvz` sowohl auf numerische als auch auf kategorielle oder nominale Variablen an.  

```{r}
housing_rec %>% 
  step_nzv(all_predictors()) -> housing_rec
```

### Zentrieren numerischer Variablen

Bei der linearen Regression ist der Erwartungswert der Zielvariable $y$ gleich dem konstanten Term $beta_0$, wenn die Prädiktoren $x_i = 0$ sind. 0 ist aber für viele Anwendungen in der Realität ein außergewöhnlicher Wert, bspw. eine Größe von 0$m$, eine Fläche von 0 $m^2$, eine Zahl von 0 Zimmern usw. Es wäre viel intuitiver, wenn $y = \beta_0$ für den Fall, dass die Prädiktoren ihren jeweiligen Durchschnittswert annehmen. Und genau das kann man erreichen, wenn man die Variablen zentriert, d.h. vom jeweiligen Wert den Durchschnitt abzieht, denn dann haben die (zentrierten) Variablen den Durchschnittswert 0.

Manchmal *skaliert* man die Variablen auch noch zusätzlich zur Zentrierung: Das bedeutet, dass man die Werte auch noch durch die Standardabweichung dividiert. In der Statistik nennt man beide Schritte zusammen *z-Transformation*. Damit sind die Verteilungen der Variablenwerte vergleichbar und die Koeffizienten $\beta_i$ sind damit auch einfacher vergleichbar. Tatsächlich führt man durch den Schritt eine Einheitenumrechnung durch und alle Koeffizienten $\beta_i$ haben dadurch dieselbe Einheit.

```{r}
housing_rec %>% step_center(all_numeric()) %>% step_scale(all_numeric()) -> housing_rec
```




### Bereinigung um stark korrelierte Variablen

Wie oben erläutert, können stark korrelierte Variablen die Robustheit des Modells und dessen Interpretation verschlechtern. Daher entfernen wir stark korrelierte numerische Variablen:
```{r}
housing_rec %>% step_corr(all_numeric(), threshold = 0.8) -> housing_rec
```



### Behandlung kategorieller Variablen
one-hot-encoding oder dummy-encoding




## Modellbildung

Lasso-Modell: Gleichzeitig Features auswählen und Modell fitten

### Trainings- und Testdaten

### Modell erstellen

## Modellinterpretation






Aus der Dokumentation zu dem Paket, die man mittels
```{r eval=FALSE, include=TRUE}
??AmesHousing
```

aufrufen kann, erfahren wir, dass wir mittels der Funktion `make_ames` eine schon etwas aufbereitete Datenbasis erhalten können:

```{r eval=FALSE, include=TRUE}
data <- make_ames()
```





## Notizen

* [Data Doku](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)
* [GeoLokationen im Artikel](http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html)
* [Beispielanalyse](http://rstudio-pubs-static.s3.amazonaws.com/247652_bb5c001d6f7642d88f9ff66ecf1e28a3.html)



